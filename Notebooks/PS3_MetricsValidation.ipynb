{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics and Validation Techniques\n",
    "\n",
    "This worksheet builds uses the k-fold validation technique to determine the hyperparameter required for Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Accuracy Check on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/spambase_2.csv',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2837, 58)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covnert X,y to numpy arrays\n",
    "X = df.iloc[:,0:57].values\n",
    "y = df.iloc[:,57].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Data using logistic regression\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9837856891082128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2783,    4],\n",
       "       [  42,    8]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions and check accuracy\n",
    "ypred = lr.predict(X)\n",
    "print(\"Accuracy =\", accuracy_score(y,ypred))\n",
    "\n",
    "confusion_matrix(y,ypred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement K-Fold Validation with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/spambase.csv',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X,y to numpy arrays\n",
    "X = df.iloc[:,0:57].values\n",
    "y = df.iloc[:,57].values.reshape(-1,1)\n",
    "\n",
    "n = y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression Class\n",
    "\n",
    "Reminder: Ridge Regression implements regularization by adding to the Loss function:\n",
    "\n",
    "$$ Loss \\rightarrow Loss + \\lambda |\\theta|^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.theta = []\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def sigmoid(self,X):\n",
    "        \n",
    "        linear_form = X@self.theta\n",
    "        h = 1/(1+np.exp(-linear_form))\n",
    "        return h\n",
    "    \n",
    "    def negative_loglikelihood(self,X,y):\n",
    "        \n",
    "        n = y.shape[0]\n",
    "        h = self.sigmoid(X)\n",
    "        if any(h>0.999):\n",
    "            idx_high = np.where(h>0.999)[0]\n",
    "            h[idx_high] = 0.999\n",
    "        elif any(h<0.001):\n",
    "            idx_low = np.where(h<0.001)[0]\n",
    "            h[idx_low] = 0.001\n",
    "        J = -(1/n)*(np.sum(y*np.log(h)) + np.sum((1-y)*np.log(1-h)))\n",
    "\n",
    "        return J\n",
    "\n",
    "    def gradient(self,X,y,lmbda):\n",
    "\n",
    "        n = y.shape[0]\n",
    "        h = self.sigmoid(X)\n",
    "        dtheta = (1/n)*X.T@(h-y)+(lmbda/n)*self.theta\n",
    "\n",
    "        return dtheta\n",
    "    \n",
    "    def fit(self,x_train,y_train,x_test,y_test,max_iter,lmbda,learning_rate,plot_flag = True):\n",
    "        \n",
    "        # Number of examples, parameters\n",
    "        n_train = y_train.shape[0]\n",
    "        n_test = y_test.shape[0]\n",
    "        n_params = x_train.shape[1]\n",
    "        \n",
    "        # Scale the data according to training data\n",
    "        x_train = self.scaler.fit_transform(x_train)\n",
    "        x_test = self.scaler.transform(x_test)\n",
    "        \n",
    "        # Add column of ones to x data\n",
    "        x_train = np.c_[np.ones(n_train),x_train]\n",
    "        x_test = np.c_[np.ones(n_test), x_test]\n",
    "        \n",
    "        # Initialize theta\n",
    "        self.theta = np.random.uniform(low=-0.1,high=0.1, size=(n_params+1,1))\n",
    "        \n",
    "        # Initialize cost vector and iteration vector for later plotting\n",
    "        iter_vec = np.array(range(max_iter))\n",
    "        cost_vec_train = np.arange(max_iter).astype('float')\n",
    "        cost_vec_test = np.arange(max_iter).astype('float')\n",
    "\n",
    "        for ii in range(max_iter):\n",
    "\n",
    "            # Calculate the gradient\n",
    "            dtheta = self.gradient(x_train,y_train,lmbda)\n",
    "\n",
    "            # Update theta\n",
    "            self.theta = self.theta - learning_rate * dtheta\n",
    "\n",
    "            # Calculate the value of the log-likelihood\n",
    "            cost_train = self.negative_loglikelihood(x_train,y_train)\n",
    "            cost_test = self.negative_loglikelihood(x_test,y_test)\n",
    "\n",
    "            # Save cost for later plotting\n",
    "            cost_vec_train[ii] = cost_train\n",
    "            cost_vec_test[ii] = cost_test\n",
    "            \n",
    "        # Plot final solution\n",
    "        if plot_flag:\n",
    "            plt.plot(iter_vec,cost_vec_test,'-r',label='Test Error')\n",
    "            plt.plot(iter_vec,cost_vec_train,'b',label='Train Error')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Log-Likelihood')\n",
    "            plt.yscale('log')\n",
    "            plt.title('lmbda = '+str(lmbda))\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    def predict(self,X,threshold):\n",
    "        \n",
    "        # Scale new data\n",
    "        X = self.scaler.transform(X)\n",
    "        \n",
    "        # Add the column of ones\n",
    "        X = np.c_[np.ones(X.shape[0]),X]\n",
    "        \n",
    "        # Transform to get new y\n",
    "        linear_form = X@self.theta\n",
    "        h = 1/(1+np.exp(-linear_form))\n",
    "        \n",
    "        # Prediction is 0s and 1s based on threshold\n",
    "        ypred = np.array([int(hi>threshold) for hi in h]).reshape(-1,1)\n",
    "        \n",
    "        return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Split Training Set and Testing Set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.5)\n",
    "\n",
    "model.fit(x_train,y_train,x_test,y_test,5000,0.1,1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.9182608695652174\n",
      "Test Accuracy = 0.9096045197740112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1307,   59],\n",
       "       [ 149,  786]], dtype=int64)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred_train = model.predict(x_train,0.5)\n",
    "ypred_test = model.predict(x_test,0.5)\n",
    "print(\"Training Accuracy =\", accuracy_score(y_train,ypred_train))\n",
    "print(\"Test Accuracy =\", accuracy_score(y_test,ypred_test))\n",
    "confusion_matrix(y_train,ypred_train)\n",
    "confusion_matrix(y_test,ypred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Folds Cross-Validation\n",
    "\n",
    "This searches for the best hyperparameters by partitioning the data into K subsets, and then leaving out one subset to use as the test set. After iterating over all K subsets, the average accuracy is calculated to determine which hyperparameters were best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data set into K-folds\n",
    "k_folds = 5\n",
    "n_data = y.shape[0]\n",
    "n_partition = int(n_data/k_folds)\n",
    "\n",
    "x_partitions = []\n",
    "y_partitions = []\n",
    "\n",
    "indices = np.arange(n_data)\n",
    "for ii in range(k_folds):\n",
    "    \n",
    "    indices_permuted = np.random.permutation(indices)\n",
    "    \n",
    "    if ii==(k_folds-1):\n",
    "        n_partition = len(indices_permuted)\n",
    "        \n",
    "    idx_partition = indices_permuted[0:n_partition]\n",
    "    indices = indices_permuted[n_partition:]\n",
    "    x_partitions.append(X[idx_partition])\n",
    "    y_partitions.append(y[idx_partition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of lmbda was:  0.1\n",
      "Accuracy:  0.9237150073171883\n"
     ]
    }
   ],
   "source": [
    "# Fit lmbda using k-folds\n",
    "lmbda_best = 0\n",
    "lmbda_list = [0,0.001,0.01,0.1,1,10,100]\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Initialize an accuracy list for each k_fold\n",
    "accuracy_list = np.arange(k_folds).astype('float')\n",
    "accuracy_best = 0\n",
    "\n",
    "# Loop through all the lmbda and fine the best one\n",
    "for lmbda in lmbda_list:\n",
    "    for jj in range(k_folds):\n",
    "\n",
    "        # Create the training and validation data\n",
    "        all_idx = list(range(k_folds))\n",
    "        test_idx = jj\n",
    "        all_idx.remove(jj)\n",
    "        train_idx = all_idx\n",
    "\n",
    "        x_test = x_partitions[test_idx]\n",
    "        y_test = y_partitions[test_idx]\n",
    "\n",
    "        x_train = x_partitions[train_idx[0]]\n",
    "        y_train = y_partitions[train_idx[0]]\n",
    "        for ii in train_idx[1::]:\n",
    "            x_train = np.vstack((x_train,x_partitions[ii]))\n",
    "            y_train = np.vstack((y_train,y_partitions[ii]))\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(x_train,y_train,x_test,y_test,3000,lmbda,1e-1,False)\n",
    "        \n",
    "        # Make predictions\n",
    "        ypred_test = model.predict(x_test,0.5)\n",
    "        accuracy_kfold_j = accuracy_score(y_test,ypred_test)\n",
    "        accuracy_list[jj] = accuracy_kfold_j\n",
    "\n",
    "    # Average over all k folds    \n",
    "    accuracy = np.mean(accuracy_list)\n",
    "    \n",
    "    # Update the best value of lmbda\n",
    "    if accuracy>accuracy_best:\n",
    "        lmbda_best = lmbda\n",
    "        accuracy_best = accuracy\n",
    "\n",
    "print(\"Best value of lmbda was: \",lmbda_best)\n",
    "print(\"Accuracy: \",accuracy_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
