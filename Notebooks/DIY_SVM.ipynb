{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries, set some settings\n",
    "\n",
    "# Useful libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Modelling and Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Black box\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Visualise\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings for the notebook\n",
    "np.random.seed(0)\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Get rid of annoying stuff\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classes\n",
    "---\n",
    "\n",
    "We have here 3 different classes that could be used for different purposes, and on different types of data. \n",
    "\n",
    "## SVMLinearModel\n",
    "---\n",
    "\n",
    "This employs: \n",
    "\n",
    "   * Linear Kernel: \n",
    "    \n",
    "        $ K(\\vec{x_i},\\vec{x_j}) = \\vec{x_i} \\cdot \\vec{x_j} $\n",
    "    \n",
    "    \n",
    "   * Batch Gradient Descent\n",
    "   * Hinge Loss Function\n",
    "    \n",
    "Pros:\n",
    "\n",
    "   * Works quicker than other methods\n",
    "   * Acceptable results in high dimensional spaces\n",
    "    \n",
    "Cons:\n",
    "\n",
    "   * Does not capture non-linearities\n",
    "    \n",
    "    \n",
    "## SVMGaussianKernel\n",
    "---\n",
    "\n",
    "This employs: \n",
    "\n",
    "   * Gaussian / Radial Basis Function Kernel:\n",
    "   \n",
    "       $ K(\\vec{x_i},\\vec{x_j}) = e^{-\\gamma (\\vec{x_i} - \\vec{x_j})^2} $\n",
    "       \n",
    "       \n",
    "   * Sequential Minimal Optimization\n",
    "   \n",
    "       * Analytically update 2 Lagrange Multipliers ($\\alpha_i$, $\\alpha_j$) simultaneously\n",
    "       * When approximately converged, $\\alpha_i$ should automatically stop changing\n",
    "       \n",
    "   * Full vectorized Kernel matrix calculation. \n",
    "       \n",
    "Pros: \n",
    "\n",
    "   * Vectorized Kernel makes this slightly faster than following method\n",
    "   * Can easily capture many non-linearities\n",
    "   \n",
    "Cons:\n",
    "\n",
    "   * Easy to overfit\n",
    "   * Slower than linear model for large amounts of data\n",
    "   * Memory intensive\n",
    "   \n",
    "   \n",
    "## SVMGaussianKernelModified\n",
    "---\n",
    "\n",
    "This employs: \n",
    "\n",
    "   * Gaussian / Radial Basis Function Kernel:\n",
    "   \n",
    "       $ K(\\vec{x_i},\\vec{x_j}) = e^{-\\gamma (\\vec{x_i} - \\vec{x_j})^2} $\n",
    "       \n",
    "       \n",
    "   * Sequential Minimal Optimization\n",
    "   \n",
    "       * Analytically update 2 Lagrange Multipliers ($\\alpha_i$, $\\alpha_j$) simultaneously\n",
    "       * When approximately converged, $\\alpha_i$ should automatically stop changing\n",
    "       \n",
    "   * Only calculates the portions of the Kernel that are needed. \n",
    "       \n",
    "Pros: \n",
    "\n",
    "   * Uses less memory than the previous method\n",
    "   * Can easily capture many non-linearities\n",
    "   \n",
    "Cons:\n",
    "\n",
    "   * Easy to overfit\n",
    "   * Slower than linear model for large amounts of data\n",
    "   * Slower than previous Gaussian method due to multiple Kernel re-calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMLinear:\n",
    "    \"\"\"\n",
    "        SVM Class that uses a Linear Kernel and (Batch) Gradient Descent on the Hinge Loss Function.\n",
    "\n",
    "        Required Parameters for fit call::\n",
    "        x = Training Data Matrix.\n",
    "        y = Training Data Classes. This should be size n x 1.\n",
    "\n",
    "        Optional Parameters: \n",
    "        C = 1.0, controls amount of 'slack' allowed. Larger C = less margin violations are allowed. \n",
    "        learning_rate = 0.005, controls speed of gradient descent.\n",
    "        max_iter = 5000, controls number of gradient descent iterations. \n",
    "\n",
    "        To Predict On New Data:\n",
    "        decision_function(x_test) = returns predictions for y, without scaling to +/- 1.\n",
    "        prediction(x_test) = returns sign(decision_function), will give actual class labels. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=1.0, learning_rate=0.005, max_iter = 5000):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_examples = 0\n",
    "        self.p_features = 0\n",
    "        self.slack_variable_c = C\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = 0\n",
    "        self.scalar_bias_term = 0\n",
    "        print(\"Initializing...\")\n",
    "        \n",
    "    def fit(self,x,y):\n",
    "        self.x = np.mat(x)\n",
    "        self.y = np.mat(y).reshape(-1,1)\n",
    "        self.n_examples = self.y.shape[0]\n",
    "        self.p_features = self.x.shape[1]\n",
    "        self.weights = np.mat(np.random.rand(self.p_features,1))\n",
    "        \n",
    "        if self.max_iter < 10000:\n",
    "            print(\"WARNING: Will automatically cut off at \",str(self.max_iter),\"iterations\")\n",
    "        print(\"Training......\")\n",
    "        \n",
    "        self.gradient_descent()\n",
    "        \n",
    "    def calculate_hinge_loss(self, test_data_x, test_data_y):\n",
    "        margins = np.multiply(test_data_y , self.decision_function(test_data_x))\n",
    "        weights_sum_of_squares = self.weights.T*self.weights\n",
    "        list_of_margin_violations = np.maximum(0, 1 - margins)\n",
    "        margin_penalty = list_of_margin_violations.sum()\n",
    "        return 0.5 * weights_sum_of_squares + self.slack_variable_c* margin_penalty\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        for ii in range(self.max_iter):\n",
    "            \n",
    "            # Grab the indices for any margins which are less than 1\n",
    "            # These are data points which violate the SVM's margin lines\n",
    "            margins = np.multiply(self.y , self.decision_function(self.x))\n",
    "            idx_of_dataset_that_violated_margin = np.where(margins < 1)[0]\n",
    "            # Update the vector of weights and the bias term\n",
    "            #Calc the gradient for W\n",
    "            gradient_of_weights = self.weights -  self.slack_variable_c*self.x[idx_of_dataset_that_violated_margin].T*self.y[idx_of_dataset_that_violated_margin]\n",
    "            # update the vector of weights\n",
    "            self.weights = self.weights - self.learning_rate * gradient_of_weights \n",
    "            # Calculate the gradient for the bias term\n",
    "            gradient_of_bias_term = - self.slack_variable_c*self.y[idx_of_dataset_that_violated_margin].sum() \n",
    "            # update the bias term\n",
    "            self.scalar_bias_term -= self.learning_rate * gradient_of_bias_term\n",
    "\n",
    "    def decision_function(self,test_data_x):\n",
    "        # Decision Function f(x)= sum_i alpha_i y_i Kernel(x_i,x) + b\n",
    "        test_data_x = np.mat(test_data_x)\n",
    "        decision = test_data_x*self.weights+self.scalar_bias_term\n",
    "        return decision\n",
    "\n",
    "    def predict(self, test_data_x):\n",
    "        decisions = self.decision_function(test_data_x)\n",
    "        prediction = np.sign(decisions)\n",
    "        return prediction\n",
    "\n",
    "    \n",
    "class SVMGaussianKernel:\n",
    "    \"\"\"\n",
    "        SVM Class that uses the Radial Basis Function/Gaussian Kernel.\n",
    "        Optimization is done using Sequential Minimal Optimization (SMO) Techniques.\n",
    "        \n",
    "        **WARNING**\n",
    "        This version calculates the FULL kernel to increase the speed of the calculations. \n",
    "        As such, this should NOT be used on large data sets (where n x x will be >> memory limitations)\n",
    "\n",
    "        Required Parameters for fit call:\n",
    "        x = Training Data Matrix.\n",
    "        y = Training Data Classes. This should be size n x 1.\n",
    "\n",
    "        Optional Parameters: \n",
    "        C = 1.0, controls amount of 'slack' allowed. Larger C = less margin violations are allowed. \n",
    "        gamma = 1.0, controls the 'width' of the kernel distributions. Larger gamma = more overfitting.\n",
    "        max_iter = 5000, controls number of run throughs of successful alpha changes (similar to gradient descent).\n",
    "        max_passes = 50, allows for multiple checks of alpha stability (due to random nature of choosing 2 alphas) \n",
    "                     before stopping\n",
    "        tol = 0.01, controls how closely alpha must be within KKT constraints to skip modifications\n",
    "        alpha_tol = 0.00001, controls how much alpha must move to consider having changed within a loop\n",
    "\n",
    "        To Predict On New Data:\n",
    "        decision_function(x_test) = returns predictions for y, without scaling to +/- 1.\n",
    "        prediction(x_test) = returns sign(decision_function), will give actual class labels. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=1.0,gamma=1.0,max_iter = 5000 , max_passes = 50 , tol = 0.01  , alpha_tol = 0.00001):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.slack_variable_c = C\n",
    "        self.kernel_hyper_param_gamma = gamma\n",
    "        self.tol = tol\n",
    "        self.max_passes = max_passes\n",
    "        self.max_iter = max_iter\n",
    "        self.threshold_for_negligibility_of_alpha_movement = alpha_tol\n",
    "        self.kernel_gaussian = 0\n",
    "        self.alpha_vector_of_lagrange_multipliers = 0\n",
    "        self.scalar_bias_term = 0\n",
    "        print(\"Initializing...\")\n",
    "        \n",
    "    def fit(self,x,y):\n",
    "        self.x = np.mat(x)\n",
    "        self.y = np.mat(y).reshape(-1,1)\n",
    "        self.kernel_gaussian = self.create_kernel(self.x,self.x,self.kernel_hyper_param_gamma)\n",
    "        self.alpha_vector_of_lagrange_multipliers = np.mat(np.zeros((np.shape(x)[0], 1)))\n",
    "        \n",
    "        i = 0\n",
    "        self.alpha_not_changed_so_increment_toward_max_passes = False\n",
    "        \n",
    "        if self.max_iter < 10000:\n",
    "            print(\"WARNING: Will automatically cut off at \",str(self.max_iter),\"iterations\")\n",
    "        \n",
    "        print(\"Training......\")\n",
    "        count = 0\n",
    "        while (i < self.max_passes) and (count<=self.max_iter):\n",
    "            # Take one pass of optimization attempts through whole data set\n",
    "            self.do_one_SMO_pass()\n",
    "            # If alpha has moved, there is still room for optimizing alpha\n",
    "            # So reset the counter towards max passes\n",
    "            if self.alpha_not_changed_so_increment_toward_max_passes:\n",
    "                i += 1\n",
    "            # If alpha has stopped moving, start counting towards max_passes\n",
    "            else:\n",
    "                i = 0\n",
    "                count+=1\n",
    "            if count%1000==0:\n",
    "                print(count)\n",
    "    \n",
    "    def create_kernel(self,x1,x2,gamma):\n",
    "        # Create a vector of all row magnitudes ^2 \n",
    "        x1_sum_squared = np.sum(np.multiply(x1,x1),1)\n",
    "        x2_sum_squared = np.sum(np.multiply(x2,x2),1)\n",
    "        # Create a matrix for x1 dot x2\n",
    "        x1_dot_x2 = x1*x2.T\n",
    "        # Calculate (xi-xj)^2 = (xi^2 + xj^2 - 2 xi dot xj)\n",
    "        x1_minus_x2_squared = x1_sum_squared + x2_sum_squared.T - 2*x1_dot_x2\n",
    "        # Create the kernel\n",
    "        kernel_gaussian = np.exp(-gamma*x1_minus_x2_squared)\n",
    "        return kernel_gaussian\n",
    "\n",
    "    def do_one_SMO_pass(self):\n",
    "        n_changed_alpha = 0\n",
    "        # Iterate through whole data set\n",
    "\n",
    "        number_of_rows_in_x = np.shape(self.x)[0]\n",
    "        for i in range(number_of_rows_in_x):\n",
    "            element_wise_product_of_y_and_alpha_vectors = np.multiply(self.y, self.alpha_vector_of_lagrange_multipliers)\n",
    "\n",
    "            # Ei = summation(alpha[m] * y[m] * k[i,m]) +b - y[m]\n",
    "            error_at_i = element_wise_product_of_y_and_alpha_vectors.T * self.kernel_gaussian[i].T\\\n",
    "                + self.scalar_bias_term - self.y[i]\n",
    "\n",
    "            #print(\"Error terms at: \", str(error_at_i))\n",
    "\n",
    "            # IF (yiEi < -tol AND alpha_i <C) OR (yiEi > tol AND alpha_i > 0):\n",
    "            if self.check_alpha_fails_to_satisfy_constraints_on_slack_variables(self.alpha_vector_of_lagrange_multipliers[i], error_at_i):\n",
    "                # Then:\n",
    "\n",
    "                # Randomly Select a j that is not equal to i by drawing random indices until we get one that isn't i\n",
    "                draw_another_number = True\n",
    "                j = int(np.random.uniform(0, number_of_rows_in_x))\n",
    "                while draw_another_number:\n",
    "                    if j != i:\n",
    "                        draw_another_number = False\n",
    "                    else:\n",
    "                        j = int(np.random.uniform(0, number_of_rows_in_x))\n",
    "\n",
    "                # Ei = summation(alpha[m] * y[m] * k[j, m]) + b - y[j]\n",
    "                error_at_j = element_wise_product_of_y_and_alpha_vectors.T * self.kernel_gaussian[j].T\\\n",
    "                    + self.scalar_bias_term - self.y[j]\n",
    "\n",
    "                old_ith_element_of_alpha_vector = self.alpha_vector_of_lagrange_multipliers[i].copy()\n",
    "                old_jth_element_of_alpha_vector = self.alpha_vector_of_lagrange_multipliers[j].copy()\n",
    "\n",
    "                # Get the upper and lower bounds for alpha\n",
    "                lower_bound = 0\n",
    "                upper_bound = 0\n",
    "\n",
    "                # IF yi != yj :\n",
    "                if self.y[i] == self.y[j]:\n",
    "                    # L = max(0,alpha_i + alpha_j - C)\n",
    "                    lower_bound = max(0,\n",
    "                                      (old_ith_element_of_alpha_vector + old_jth_element_of_alpha_vector)\n",
    "                                      - self.slack_variable_c)\n",
    "                    # H = min(C, alpha_i + alpha_j)\n",
    "                    upper_bound = min(self.slack_variable_c,\n",
    "                                      (old_ith_element_of_alpha_vector + old_jth_element_of_alpha_vector))\n",
    "\n",
    "                else:\n",
    "                    # L = max(0,alpha_j - alpha_i)\n",
    "                    lower_bound = \\\n",
    "                        max(0, (old_jth_element_of_alpha_vector - old_ith_element_of_alpha_vector))\n",
    "                    # H = min(C, C + (alpha_j - alpha_i))\n",
    "                    upper_bound = \\\n",
    "                        min(self.slack_variable_c,\n",
    "                            ((old_jth_element_of_alpha_vector - old_ith_element_of_alpha_vector) + self.slack_variable_c))\n",
    "\n",
    "                # eita = 2K[i,j] - K[i,i] - k[j,j]\n",
    "                second_derivative_of_dual = 2.0*self.kernel_gaussian[i,j] - self.kernel_gaussian[i,i]\\\n",
    "                    -self.kernel_gaussian[j,j]\n",
    "\n",
    "                if lower_bound != upper_bound and second_derivative_of_dual < 0:\n",
    "                    succeeded_at_update_attempt = \\\n",
    "                        self.attempt_to_update_alphas_at_i_and_j(i, j, error_at_i, error_at_j,\n",
    "                                                                 second_derivative_of_dual,\n",
    "                                                                 lower_bound,upper_bound,\n",
    "                                                                 old_ith_element_of_alpha_vector,\n",
    "                                                                 old_jth_element_of_alpha_vector)\n",
    "                    if succeeded_at_update_attempt:\n",
    "                        n_changed_alpha += 1\n",
    "\n",
    "        # If alpha has moved, there is still room for optimizing alpha\n",
    "        # So reset the counter towards max passes\n",
    "        if n_changed_alpha > 0:\n",
    "            self.alpha_not_changed_so_increment_toward_max_passes = False\n",
    "        # If alpha has stopped moving, start counting towards towards max_passes\n",
    "        else:\n",
    "            self.alpha_not_changed_so_increment_toward_max_passes = True\n",
    "        return\n",
    "\n",
    "    def attempt_to_update_alphas_at_i_and_j(self, i, j, error_at_i, error_at_j, eita,\n",
    "                                            lower_bound, upper_bound, old_alpha_i, old_alpha_j):\n",
    "        succeeded_at_attempt_to_optimize = False\n",
    "\n",
    "        # From Lindsay PDF: distance to move = yj(Ei-Ej)/eita\n",
    "        how_far_to_move_alpha_j = self.y[j] * (error_at_i - error_at_j) / eita\n",
    "\n",
    "        self.alpha_vector_of_lagrange_multipliers[j] -= how_far_to_move_alpha_j\n",
    "\n",
    "        # IF alpha j has tried to go outside the bounds, reel it back in\n",
    "        if self.alpha_vector_of_lagrange_multipliers[j] < lower_bound:\n",
    "            self.alpha_vector_of_lagrange_multipliers[j] = lower_bound\n",
    "        if self.alpha_vector_of_lagrange_multipliers[j] > upper_bound:\n",
    "            self.alpha_vector_of_lagrange_multipliers[j] = upper_bound\n",
    "\n",
    "        alpha_j_movement_since_old_alpha_j = self.alpha_vector_of_lagrange_multipliers[j] - old_alpha_j\n",
    "\n",
    "        # If alpha only moved by an insignificant amount, don't count it as a movement\n",
    "        if abs(alpha_j_movement_since_old_alpha_j) >= self.threshold_for_negligibility_of_alpha_movement:\n",
    "            # Else: From Lindsay PDF: alpha = old_alpha_i + yiyi + (old_alpha_j - new_alpha_j)\n",
    "            self.alpha_vector_of_lagrange_multipliers[i] += self.y[j] * self.y[i] * (\n",
    "                    old_alpha_j - self.alpha_vector_of_lagrange_multipliers[j])\n",
    "\n",
    "            #Update the bias term b\n",
    "\n",
    "            last_movement_of_alpha_i = self.alpha_vector_of_lagrange_multipliers[i] - old_alpha_i\n",
    "            last_movement_of_alpha_j = self.alpha_vector_of_lagrange_multipliers[j] - old_alpha_j\n",
    "\n",
    "            # From Lindsay PDF: definitions for bi and bj\n",
    "            b1 = self.scalar_bias_term - error_at_i - self.y[i] * last_movement_of_alpha_i * self.kernel_gaussian[i,i] \\\n",
    "                 - self.y[j] * last_movement_of_alpha_j * self.kernel_gaussian[i,j]\n",
    "\n",
    "            b2 = self.scalar_bias_term - error_at_j - self.y[i] * last_movement_of_alpha_i * self.kernel_gaussian[i,j] \\\n",
    "                 - self.y[j] * last_movement_of_alpha_j * self.kernel_gaussian[j,j]\n",
    "\n",
    "            if (0 < self.alpha_vector_of_lagrange_multipliers[i]) and \\\n",
    "                    (self.slack_variable_c > self.alpha_vector_of_lagrange_multipliers[i]):\n",
    "                self.scalar_bias_term = b1\n",
    "            elif (0 < self.alpha_vector_of_lagrange_multipliers[j]) and \\\n",
    "                    (self.slack_variable_c > self.alpha_vector_of_lagrange_multipliers[j]):\n",
    "                self.scalar_bias_term = b2\n",
    "            else:\n",
    "                # From Lindsay PDF: b = (bi+bj)/2 bottom of page 3\n",
    "                self.scalar_bias_term = (b1 + b2) / 2.0\n",
    "\n",
    "\n",
    "            succeeded_at_attempt_to_optimize = True\n",
    "\n",
    "        return succeeded_at_attempt_to_optimize\n",
    "\n",
    "    def check_alpha_fails_to_satisfy_constraints_on_slack_variables(self, alpha, E):\n",
    "        if (np.abs(E) < self.tol) and (alpha > 0):\n",
    "            return True\n",
    "        elif alpha < self.slack_variable_c and np.abs(E) > self.tol:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def decision_function(self,test_data_x):\n",
    "        test_data_x = np.mat(test_data_x)\n",
    "        kernel_for_test = self.create_kernel(test_data_x,self.x,self.kernel_hyper_param_gamma)\n",
    "        element_wise_product_of_y_and_alpha_vectors = np.multiply(self.y, self.alpha_vector_of_lagrange_multipliers)\n",
    "        decisions = kernel_for_test*element_wise_product_of_y_and_alpha_vectors + self.scalar_bias_term\n",
    "        return decisions\n",
    "\n",
    "    def predict(self, test_data_x):\n",
    "        decisions = self.decision_function(test_data_x)\n",
    "        prediction = np.sign(decisions)\n",
    "        return prediction\n",
    "    \n",
    "    def load_weights_and_settings(self,name):\n",
    "        print(\"Loading pre-trained file and settings......\")\n",
    "        file_input_stream = open(name, 'rb')\n",
    "        trained_weights = pickle.load(file_input_stream)\n",
    "        file_input_stream.close()          \n",
    "        self.__dict__.update(trained_weights)\n",
    "        print(\"Model loaded successfully\")\n",
    "        print(\"Gamma = \",self.kernel_hyper_param_gamma)\n",
    "        print(\"max_iter = \",self.max_iter)\n",
    "        \n",
    "    def save_weights_and_settings(self,name_to_use):\n",
    "        print(\"Saving Model.....\")\n",
    "        file_input_stream = open(name_to_use, 'wb')\n",
    "        pickle.dump(self.__dict__, file_input_stream, 2)\n",
    "        file_input_stream.close()\n",
    "        print(\"Model saved as: \",name_to_use)\n",
    "        \n",
    "\n",
    "class SVMGaussianKernelModified:\n",
    "    \"\"\"\n",
    "        SVM Class that uses the Radial Basis Function/Gaussian Kernel.\n",
    "        Optimization is done using Sequential Minimal Optimization (SMO) Techniques.\n",
    "        \n",
    "        **WARNING**\n",
    "        This version calculates one column of kernel to decrease the memory load of the calculations. \n",
    "        As such, this will likely move slowly on large datasets.\n",
    "\n",
    "        Required Parameters for fit call:\n",
    "        x = Training Data Matrix.\n",
    "        y = Training Data Classes. This should be size n x 1.\n",
    "\n",
    "        Optional Parameters: \n",
    "        C = 1.0, controls amount of 'slack' allowed. Larger C = less margin violations are allowed. \n",
    "        gamma = 1.0, controls the 'width' of the kernel distributions. Larger gamma = more overfitting.\n",
    "        max_iter = 5000, controls number of run throughs of successful alpha changes (similar to gradient descent).\n",
    "        max_passes = 50, allows for multiple checks of alpha stability (due to random nature of choosing 2 alphas) \n",
    "                     before stopping\n",
    "        tol = 0.01, controls how closely alpha must be within KKT constraints to skip modifications\n",
    "        alpha_tol = 0.00001, controls how much alpha must move to consider having changed within a loop\n",
    "\n",
    "        To Predict On New Data:\n",
    "        decision_function(x_test) = returns predictions for y, without scaling to +/- 1.\n",
    "        prediction(x_test) = returns sign(decision_function), will give actual class labels. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=1.0,gamma=1.0,max_iter = 5000 , max_passes = 20 , tol = 0.01  , alpha_tol = 0.00001):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.slack_variable_c = C\n",
    "        self.kernel_hyper_param_gamma = gamma\n",
    "        self.tol = tol\n",
    "        self.max_passes = max_passes\n",
    "        self.max_iter = max_iter\n",
    "        self.threshold_for_negligibility_of_alpha_movement = alpha_tol\n",
    "        self.alpha_vector_of_lagrange_multipliers = 0\n",
    "        self.scalar_bias_term = 0\n",
    "        print(\"Initializing...\")\n",
    "        \n",
    "    def fit(self,x,y):\n",
    "        self.x = np.mat(x)\n",
    "        self.y = np.mat(y).reshape(-1,1)\n",
    "        self.alpha_vector_of_lagrange_multipliers = np.mat(np.zeros((np.shape(x)[0], 1)))\n",
    "        \n",
    "        i = 0\n",
    "        self.alpha_not_changed_so_increment_toward_max_passes = False\n",
    "        \n",
    "        if self.max_iter < 10000:\n",
    "            print(\"WARNING: Will automatically cut off at \",str(self.max_iter),\"iterations\")\n",
    "        \n",
    "        print(\"Training......\")\n",
    "        count = 0\n",
    "        while (i < self.max_passes) and (count<=self.max_iter):\n",
    "            # Take one pass of optimization attempts through whole data set\n",
    "            self.do_one_SMO_pass()\n",
    "            # If alpha has moved, there is still room for optimizing alpha\n",
    "            # So reset the counter towards max passes\n",
    "            if self.alpha_not_changed_so_increment_toward_max_passes:\n",
    "                i += 1\n",
    "            # If alpha has stopped moving, start counting towards max_passes\n",
    "            else:\n",
    "                i = 0\n",
    "                count+=1\n",
    "            if count%1000==0:\n",
    "                print(count)\n",
    "    \n",
    "    def create_kernel(self,x1,x2):\n",
    "        # Create a vector of all row magnitudes ^2 \n",
    "        x1_sum_squared = np.sum(np.multiply(x1,x1),1)\n",
    "        x2_sum_squared = np.sum(np.multiply(x2,x2),1)\n",
    "        # Create a matrix for x1 dot x2\n",
    "        x1_dot_x2 = x1*x2.T\n",
    "        # Calculate (xi-xj)^2 = (xi^2 + xj^2 - 2 xi dot xj)\n",
    "        x1_minus_x2_squared = x1_sum_squared + x2_sum_squared.T - 2*x1_dot_x2\n",
    "        # Create the kernel\n",
    "        kernel_gaussian = np.exp(-self.kernel_hyper_param_gamma*x1_minus_x2_squared)\n",
    "        return kernel_gaussian\n",
    "    \n",
    "    def calculate_error_i(self,idx):\n",
    "        # Ei = summation(alpha[m] * y[m] * k[i,m]) +b - y[m]\n",
    "        element_wise_product_of_y_and_alpha_vectors = np.multiply(self.y, self.alpha_vector_of_lagrange_multipliers)\n",
    "        # Calculate single column of the kernel matrix as needed\n",
    "        kernel_m_i = self.create_kernel(self.x,self.x[idx])\n",
    "       # print(kernel_m_i)\n",
    "        #print(element_wise_product_of_y_and_alpha_vectors.T.shape, kernel_m_i.shape)\n",
    "        error_at_i = element_wise_product_of_y_and_alpha_vectors.T @ kernel_m_i\n",
    "        #print(error_at_i)\n",
    "        error_at_i = error_at_i + self.scalar_bias_term - self.y[idx]\n",
    "        return error_at_i\n",
    "\n",
    "    def do_one_SMO_pass(self):\n",
    "        n_changed_alpha = 0\n",
    "        # Iterate through whole data set\n",
    "\n",
    "        number_of_rows_in_x = np.shape(self.x)[0]\n",
    "        for i in range(number_of_rows_in_x):\n",
    "            #element_wise_product_of_y_and_alpha_vectors = np.multiply(self.y, self.alpha_vector_of_lagrange_multipliers)\n",
    "\n",
    "            # Ei = summation(alpha[m] * y[m] * k[i,m]) +b - y[m]\n",
    "            error_at_i = self.calculate_error_i(i)\n",
    "\n",
    "            #print(\"Error terms at: \", str(error_at_i))\n",
    "\n",
    "            # IF (yiEi < -tol AND alpha_i <C) OR (yiEi > tol AND alpha_i > 0):\n",
    "            if self.check_alpha_fails_to_satisfy_constraints_on_slack_variables(self.alpha_vector_of_lagrange_multipliers[i], error_at_i):\n",
    "                # Then:\n",
    "\n",
    "                # Randomly Select a j that is not equal to i by drawing random indices until we get one that isn't i\n",
    "                draw_another_number = True\n",
    "                j = int(np.random.uniform(0, number_of_rows_in_x))\n",
    "                while draw_another_number:\n",
    "                    if j != i:\n",
    "                        draw_another_number = False\n",
    "                    else:\n",
    "                        j = int(np.random.uniform(0, number_of_rows_in_x))\n",
    "\n",
    "                # Ei = summation(alpha[m] * y[m] * k[j, m]) + b - y[j]\n",
    "                error_at_j = self.calculate_error_i(j)\n",
    "                \n",
    "                old_ith_element_of_alpha_vector = self.alpha_vector_of_lagrange_multipliers[i].copy()\n",
    "                old_jth_element_of_alpha_vector = self.alpha_vector_of_lagrange_multipliers[j].copy()\n",
    "\n",
    "                # Get the upper and lower bounds for alpha\n",
    "                lower_bound = 0\n",
    "                upper_bound = 0\n",
    "\n",
    "                # IF yi != yj :\n",
    "                if self.y[i] == self.y[j]:\n",
    "                    # L = max(0,alpha_i + alpha_j - C)\n",
    "                    lower_bound = max(0,\n",
    "                                      (old_ith_element_of_alpha_vector + old_jth_element_of_alpha_vector)\n",
    "                                      - self.slack_variable_c)\n",
    "                    # H = min(C, alpha_i + alpha_j)\n",
    "                    upper_bound = min(self.slack_variable_c,\n",
    "                                      (old_ith_element_of_alpha_vector + old_jth_element_of_alpha_vector))\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # L = max(0,alpha_j - alpha_i)\n",
    "                    lower_bound = \\\n",
    "                        max(0, (old_jth_element_of_alpha_vector - old_ith_element_of_alpha_vector))\n",
    "                    # H = min(C, C + (alpha_j - alpha_i))\n",
    "                    upper_bound = \\\n",
    "                        min(self.slack_variable_c,\n",
    "                            ((old_jth_element_of_alpha_vector - old_ith_element_of_alpha_vector) + self.slack_variable_c))\n",
    "\n",
    "                # eita = 2K[i,j] - K[i,i] - k[j,j]\n",
    "                kernel_i_j = self.create_kernel(self.x[i],self.x[j])[0,0]\n",
    "                kernel_i_i = 1\n",
    "                kernel_j_j = 1\n",
    "                second_derivative_of_dual = 2.0*kernel_i_j - kernel_i_i\\\n",
    "                    -kernel_j_j\n",
    "\n",
    "                if lower_bound != upper_bound and second_derivative_of_dual < 0:\n",
    "                    succeeded_at_update_attempt = \\\n",
    "                        self.attempt_to_update_alphas_at_i_and_j(i, j, error_at_i, error_at_j,\n",
    "                                                                 second_derivative_of_dual,\n",
    "                                                                 lower_bound,upper_bound,\n",
    "                                                                 old_ith_element_of_alpha_vector,\n",
    "                                                                 old_jth_element_of_alpha_vector)\n",
    "                    if succeeded_at_update_attempt:\n",
    "                        n_changed_alpha += 1\n",
    "\n",
    "        # If alpha has moved, there is still room for optimizing alpha\n",
    "        # So reset the counter towards max passes\n",
    "        if n_changed_alpha > 0:\n",
    "            self.alpha_not_changed_so_increment_toward_max_passes = False\n",
    "        # If alpha has stopped moving, start counting towards towards max_passes\n",
    "        else:\n",
    "            self.alpha_not_changed_so_increment_toward_max_passes = True\n",
    "        return\n",
    "\n",
    "    def attempt_to_update_alphas_at_i_and_j(self, i, j, error_at_i, error_at_j, eita,\n",
    "                                            lower_bound, upper_bound, old_alpha_i, old_alpha_j):\n",
    "        succeeded_at_attempt_to_optimize = False\n",
    "        kernel_i_j = self.create_kernel(self.x[i],self.x[j])[0,0]\n",
    "        kernel_i_i = 1\n",
    "        kernel_j_j = 1\n",
    "        # From Lindsay PDF: distance to move = yj(Ei-Ej)/eita\n",
    "        how_far_to_move_alpha_j = self.y[j] * (error_at_i - error_at_j) / eita\n",
    "\n",
    "        self.alpha_vector_of_lagrange_multipliers[j] -= how_far_to_move_alpha_j\n",
    "\n",
    "        # IF alpha j has tried to go outside the bounds, reel it back in\n",
    "        if self.alpha_vector_of_lagrange_multipliers[j] < lower_bound:\n",
    "            self.alpha_vector_of_lagrange_multipliers[j] = lower_bound\n",
    "        if self.alpha_vector_of_lagrange_multipliers[j] > upper_bound:\n",
    "            self.alpha_vector_of_lagrange_multipliers[j] = upper_bound\n",
    "\n",
    "        alpha_j_movement_since_old_alpha_j = self.alpha_vector_of_lagrange_multipliers[j] - old_alpha_j\n",
    "\n",
    "        # If alpha only moved by an insignificant amount, don't count it as a movement\n",
    "        if abs(alpha_j_movement_since_old_alpha_j) >= self.threshold_for_negligibility_of_alpha_movement:\n",
    "            # Else: From Lindsay PDF: alpha = old_alpha_i + yiyi + (old_alpha_j - new_alpha_j)\n",
    "            self.alpha_vector_of_lagrange_multipliers[i] += self.y[j] * self.y[i] * (\n",
    "                    old_alpha_j - self.alpha_vector_of_lagrange_multipliers[j])\n",
    "\n",
    "            #Update the bias term b\n",
    "\n",
    "            last_movement_of_alpha_i = self.alpha_vector_of_lagrange_multipliers[i] - old_alpha_i\n",
    "            last_movement_of_alpha_j = self.alpha_vector_of_lagrange_multipliers[j] - old_alpha_j\n",
    "\n",
    "            # From Lindsay PDF: definitions for bi and bj\n",
    "            b1 = self.scalar_bias_term - error_at_i - self.y[i] * last_movement_of_alpha_i * kernel_i_i \\\n",
    "                 - self.y[j] * last_movement_of_alpha_j * kernel_i_j\n",
    "\n",
    "            b2 = self.scalar_bias_term - error_at_j - self.y[i] * last_movement_of_alpha_i * kernel_i_j \\\n",
    "                 - self.y[j] * last_movement_of_alpha_j * kernel_j_j\n",
    "\n",
    "            if (0 < self.alpha_vector_of_lagrange_multipliers[i]) and \\\n",
    "                    (self.slack_variable_c > self.alpha_vector_of_lagrange_multipliers[i]):\n",
    "                self.scalar_bias_term = b1\n",
    "            elif (0 < self.alpha_vector_of_lagrange_multipliers[j]) and \\\n",
    "                    (self.slack_variable_c > self.alpha_vector_of_lagrange_multipliers[j]):\n",
    "                self.scalar_bias_term = b2\n",
    "            else:\n",
    "                # From Lindsay PDF: b = (bi+bj)/2 bottom of page 3\n",
    "                self.scalar_bias_term = (b1 + b2) / 2.0\n",
    "\n",
    "\n",
    "            succeeded_at_attempt_to_optimize = True\n",
    "\n",
    "        return succeeded_at_attempt_to_optimize\n",
    "\n",
    "    def check_alpha_fails_to_satisfy_constraints_on_slack_variables(self, alpha, E):\n",
    "        #print(E)\n",
    "        #print(alpha)\n",
    "        if (np.abs(E) < self.tol) and (alpha > 0):\n",
    "            return True\n",
    "        elif alpha < self.slack_variable_c and np.abs(E) > self.tol:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def decision_function(self,test_data_x):\n",
    "        # Decision Function f(x)= sum_i alpha_i y_i Kernel(x_i,x) + b\n",
    "        test_data_x = np.mat(test_data_x)\n",
    "        n_test = test_data_x.shape[0]\n",
    "        n_fit = self.x.shape[0]\n",
    "        \n",
    "        element_wise_product_of_y_and_alpha_vectors = np.multiply(self.y, self.alpha_vector_of_lagrange_multipliers)\n",
    "        decisions = np.mat(np.zeros(n_test)).T\n",
    "        for jj in range(n_test):\n",
    "            kernel_i_j = self.create_kernel(self.x,test_data_x[jj])\n",
    "            decisions[jj] = element_wise_product_of_y_and_alpha_vectors.T*kernel_i_j\n",
    "        \n",
    "        decisions = decisions +  self.scalar_bias_term\n",
    "        return decisions\n",
    "\n",
    "    def predict(self, test_data_x):\n",
    "        decisions = self.decision_function(test_data_x)\n",
    "        prediction = np.sign(decisions)\n",
    "        return prediction\n",
    "    \n",
    "    def load_weights_and_settings(self,name):\n",
    "        print(\"Loading pre-trained file and settings......\")\n",
    "        file_input_stream = open(name, 'rb')\n",
    "        trained_weights = pickle.load(file_input_stream)\n",
    "        file_input_stream.close()          \n",
    "        self.__dict__.update(trained_weights)\n",
    "        print(\"Model loaded successfully\")\n",
    "        print(\"Gamma = \",self.kernel_hyper_param_gamma)\n",
    "        print(\"max_iter = \",self.max_iter)\n",
    "        \n",
    "    def save_weights_and_settings(self,name_to_use):\n",
    "        print(\"Saving Model.....\")\n",
    "        file_input_stream = open(name_to_use, 'wb')\n",
    "        pickle.dump(self.__dict__, file_input_stream, 2)\n",
    "        file_input_stream.close()\n",
    "        print(\"Model saved as: \",name_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Classes\n",
    "---\n",
    "\n",
    "The next few classes will help us to assess and validate our model. \n",
    "\n",
    "## ModelMetrics\n",
    "\n",
    "This simply returns a bunch of useful classification metrics:\n",
    "\n",
    "   * Accuracy\n",
    "   * Confusion Matrix:\n",
    "   \n",
    "       $ \\left( \\begin{array}{cc}\n",
    "            \\text{True Negative} & \\text{False Positive} \\\\\n",
    "            \\text{False Negative} & \\text{True Positive} \\\\\n",
    "            \\end{array} \\right) $\n",
    "            \n",
    "            \n",
    "   * ROC Curve + AUC\n",
    "       * Most useful when classes are fairly balanced, otherwise it can give a skewed representation. \n",
    "       * Plot of True Positive Rate vs False Positive Rate\n",
    "       * True Positive Rate = Sensitivity = Recall = Probability of Detection\n",
    "       * Measure of how many are positive that should be.\n",
    "       \n",
    "           * $ \\frac{TP}{TP + FN} = \\frac{TP}{True P} $ \n",
    "           \n",
    "       * False Positive Rate = 1 - Specificity \n",
    "       * Measure of how many are positive that shouldn't be. \n",
    "       \n",
    "           * $ \\frac{FP}{TN + FN} = \\frac{FP}{True N} $\n",
    "           \n",
    "   * Precision Recall Curve + PR Metrics:\n",
    "       * More realistic representation of accuracy when classes are skewed. \n",
    "       * Precision = how many are predicted positive correctly out of how many are predicted positive.\n",
    "       \n",
    "           * $ \\frac{TP}{TP + FP} = \\frac{TP}{Predict P} $  \n",
    "           \n",
    "       * Recall = Sensitivity = how many are predicted positive out of how many are actually positive. \n",
    "       \n",
    "           * $ \\frac{TP}{TP + FN} = \\frac{TP}{True P}$ \n",
    "           \n",
    "       * F1 Score = harmonic average of precision and recall, helps to measure skewed data. \n",
    "       \n",
    "           * $ \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}} $    \n",
    "           \n",
    "       * AUC of PR Curve\n",
    "       * Average Precision\n",
    "       \n",
    "## kFoldsTest\n",
    "\n",
    "This will do a k folds test on a classification method:\n",
    "\n",
    "* Split data up into K partitions\n",
    "* Leave one partition out each training set\n",
    "* Train on K-1 partitions, test on the last\n",
    "* Average overall metric score over all K-folds\n",
    "* If that is the best metric score for a given hyperparameter set, save them and the corresponding model\n",
    "\n",
    "## Bootstrap\n",
    "\n",
    "This creates a bootstrap model:\n",
    "\n",
    "* Is told amount of positive and negative data to include\n",
    "* Is told number of models to put in the bag\n",
    "* Randomly selects positive and negative training data for each model\n",
    "* Fits to each randomized data set, adds to the bag\n",
    "* Can do predictions at the end based on average prediction of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMetrics():\n",
    "    \"\"\"\n",
    "        ModelMetrics class that will provide many different metrics for a given model outputs. \n",
    "\n",
    "        Required Parameters:\n",
    "        y_test = Test Data Classes. Should be either (-1,1) or (0,1).\n",
    "        y_pred = Predictions for the \"probability\" of y being in the positive class. Should be in range (-inf,inf)\n",
    "        y_prob = Test Data Predictions from the model. Should be same format as y_test. \n",
    "\n",
    "        Useful functions:\n",
    "        all_metrics() = returns metrics including: class imbalance, \n",
    "                                                   accuracy,\n",
    "                                                   confusion matrix,\n",
    "                                                   ROC/AUC,\n",
    "                                                   Precision/Recall Curve, F1 score, AUC, Average Precision\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,y_test,y_pred,y_prob):\n",
    "        self.y_test = np.array(y_test).reshape(-1)\n",
    "        self.y_pred = np.array(y_pred).reshape(-1)\n",
    "        self.y_prob = np.array(y_prob).reshape(-1)\n",
    "        \n",
    "    def all_metrics(self):\n",
    "        print('')\n",
    "        self.imbalance()\n",
    "        print('')\n",
    "        self.accuracy()\n",
    "        print('')\n",
    "        self.print_cm()\n",
    "        print('')\n",
    "        self.roc_metrics()\n",
    "        print('')\n",
    "        self.pr_metrics()\n",
    "        \n",
    "    def roc_metrics(self):\n",
    "        self.roc_plot()\n",
    "        self.print_auc_roc()\n",
    "        \n",
    "    def pr_metrics(self):\n",
    "        self.plot_pr()\n",
    "        self.print_f1()\n",
    "        self.print_auc_pr()\n",
    "        self.print_ap()\n",
    "        \n",
    "    # Measure Imbalance\n",
    "    def imbalance(self):\n",
    "        num_examples = len(self.y_test)\n",
    "        num_neg = np.sum(self.y_test==-1)\n",
    "        num_pos = np.sum(self.y_test==+1)\n",
    "        print(\"Number of -1 cases = \",num_neg)\n",
    "        print(\"Number of +1 cases = \",num_pos)\n",
    "        print('The ratio of Positve to Negative cases is {:0.3f}'.format(num_pos/num_neg))\n",
    "        \n",
    "        print('')\n",
    "        if num_neg/num_pos >= 2:\n",
    "            print('Number of Negative Classes is quite large. Recommend using the Precision-Recall Metrics.')\n",
    "        elif num_pos/num_neg >=2:\n",
    "            print('Number of Positive Classes is quite large. Recommend using the Precision-Recall Metrics.')\n",
    "        else:\n",
    "            print('The classes seem balanced. ROC metrics should be acceptable.')\n",
    "        print('')\n",
    "    \n",
    "    # Overall accuracy\n",
    "    def accuracy(self):\n",
    "        acc = accuracy_score(self.y_test,self.y_pred)\n",
    "        print('Overall Accuracy = {:0.3f}'.format(acc))\n",
    "        return acc\n",
    "        \n",
    "    # Confusion Matrix\n",
    "    def print_cm(self):\n",
    "        cm = confusion_matrix(self.y_test,self.y_pred)\n",
    "        true_negative, false_positive, false_negative, true_positive = cm.ravel()\n",
    "        print('           CONFUSION MATRIX')\n",
    "        print('                 y_pred')\n",
    "        print('          {:^10d}{:^10d}'.format(-1,1))\n",
    "        print('           --------------------')\n",
    "        print('y_test -1 |{:^10d}{:^10d}|'.format(true_negative,false_positive))\n",
    "        print('        1 |{:^10d}{:^10d}|'.format(false_negative,true_positive))\n",
    "        print('           --------------------')\n",
    "        return cm\n",
    "        \n",
    "    # ROC Curve\n",
    "    def roc_plot(self):\n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(self.y_test,self.y_prob)\n",
    "        plt.plot(false_positive_rate,true_positive_rate,'-b',label='Model')\n",
    "        plt.plot([0,1],[0,1],'-k',label='Guess')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlim([-0.02,1.0])\n",
    "        plt.ylim([0.0,1.02])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # AUC of ROC\n",
    "    def print_auc_roc(self):\n",
    "        auc1 = roc_auc_score(self.y_test,self.y_prob)\n",
    "        print('AUC = {:0.3f}'.format(auc1))\n",
    "        return auc1\n",
    "\n",
    "    ## These next few are more useful for imbalanced classes \n",
    "\n",
    "    # Precision - Recall Curve\n",
    "    def plot_pr(self):\n",
    "        precision, recall, thresholds = precision_recall_curve(self.y_test, self.y_prob)\n",
    "        plt.plot(recall,precision,'-b',label='Model')\n",
    "        plt.plot([0,1],[0.5,0.5],'-k',label='Guess')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlim([-0.02,1.0])\n",
    "        plt.ylim([0.0,1.02])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # F1 Score\n",
    "    def print_f1(self):\n",
    "        f1 = f1_score(self.y_test,self.y_pred)\n",
    "        print(\"F1 Score = {:0.3f}\".format(f1))\n",
    "        return f1\n",
    "\n",
    "    # AUC for PR\n",
    "    def print_auc_pr(self):\n",
    "        precision, recall, thresholds = precision_recall_curve(self.y_test, self.y_prob)\n",
    "        auc2 = auc(recall,precision)\n",
    "        print(\"AUC for Precision-Recall: {:0.3f}\".format(auc2))\n",
    "        return auc2\n",
    "\n",
    "    # Average precision\n",
    "    def print_ap(self):\n",
    "        average_precision = average_precision_score(self.y_test,self.y_prob)\n",
    "        print(\"Average Precision = {:0.3f}\".format(average_precision))\n",
    "        return average_precision\n",
    "    \n",
    "\n",
    "class kFoldsTest():\n",
    "    \"\"\"\n",
    "        kFoldsTest class that will provide the best C, gamma parameters for an SVM. \n",
    "\n",
    "        Required Parameters:\n",
    "        x = Input Data\n",
    "        y = Input Classes\n",
    "        \n",
    "        Optional Parameters:\n",
    "        n_splits = 10, number of k-folds to do\n",
    "        metric = accuracy_score, metric to use to decide on 'best' model \n",
    "                 (must be of form metric(y_test,y_pred)\n",
    "\n",
    "        Useful functions:\n",
    "        get_best(C_list,gamma_list,className,max_iter = 5000)\n",
    "            returns best C, gamma values for a gaussian SVM\n",
    "            ANY SVM class that uses C, gamma, and max_iter as variables AND\n",
    "            calls class.fit(x,y), class.predict(x) will work\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,x,y,n_splits = 10,metric = accuracy_score):\n",
    "        self.x_data = x\n",
    "        self.y_data = y\n",
    "        self.n_splits = n_splits\n",
    "        self.kf = KFold(n_splits=n_splits,shuffle=True)\n",
    "        self.kf.get_n_splits(self.x_data)\n",
    "        self.metric = metric\n",
    "        self.C_best = 1\n",
    "        self.gamma_best = 1\n",
    "        self.acc_best = 0\n",
    "        self.model_best = 0\n",
    "\n",
    "    def get_best(self,C_list,gamma_list,className,max_iter = 5000):\n",
    "\n",
    "        for C in C_list:\n",
    "            for gamma in gamma_list:\n",
    "                # For each value of C and gamma, come up with a new value, and compare with the best current accuracy\n",
    "                print('C = {:0.2f} and gamma = {:0.2f}'.format(C,gamma))\n",
    "                acc_list = np.empty(self.n_splits)\n",
    "                count = 0\n",
    "                for train_index, test_index in self.kf.split(self.x_data):   \n",
    "                    x_train, x_test = self.x_data[train_index], self.x_data[test_index]\n",
    "                    y_train, y_test = self.y_data[train_index], self.y_data[test_index]\n",
    "                    model = className(C = C, gamma = gamma, max_iter = max_iter)\n",
    "                    model.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    accuracy_k = self.metric(y_test,y_pred)\n",
    "                    acc_list[count] = accuracy_k\n",
    "                    count+=1\n",
    "\n",
    "                accuracy = np.mean(acc_list)\n",
    "                print('Done. \"Accuracy\" was {:0.2f}%\\n'.format(100*accuracy))\n",
    "                if accuracy>self.acc_best:\n",
    "                    self.acc_best=accuracy\n",
    "                    self.gamma_best=gamma\n",
    "                    self.C_best = C\n",
    "                    self.model_best = model\n",
    "\n",
    "        print('The best value of C was ',self.C_best)\n",
    "        print('The best value of gamma was ',self.gamma_best)\n",
    "        return self.C_best, self.gamma_best, self.model_best\n",
    "    \n",
    "    \n",
    "class Bootstrap():\n",
    "    \"\"\"\n",
    "        Bootstrap class that will produce a set of models to average over. \n",
    "\n",
    "        Optional Parameters:\n",
    "        n_train_pos = 1000, number of positive examples to include\n",
    "        n_train_neg = 10000, number of negative examples to include\n",
    "        n_models = 15, number of models to create (should be odd)\n",
    "        className = SVMGaussianKernelModified, name of class to fit\n",
    "        gamma = 1.0\n",
    "        C = 1.0\n",
    "        max_iter = 500\n",
    "\n",
    "        Useful functions:\n",
    "        fit(df,xlist,target) = fits a dataframe with X taken from columns in xlist, y has column name target\n",
    "        predict(x_test) = makes an overall prediction for what the final output should be\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,n_train_pos = 1000,n_train_neg = 10000,n_models = 15, \\\n",
    "                 className = SVMGaussianKernelModified, gamma = 1.0, C = 1.0, max_iter = 500):\n",
    "        self.n_train_pos = n_train_pos\n",
    "        self.n_train_neg = n_train_neg\n",
    "        self.n_models = n_models\n",
    "        self.className = className\n",
    "        self.gamma = gamma\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self,df, xlist, target):\n",
    "        # Split into 2 df with only +/- \n",
    "        # xlist is a list of columns to include in the x table\n",
    "        # target is the name of the row that has the output Y column\n",
    "        df_pos = df[df[target] == +1]\n",
    "        df_neg = df[df[target] == -1]\n",
    "        n_all_pos = df_pos.shape[0]\n",
    "        n_all_neg = df_neg.shape[0]\n",
    "        \n",
    "        # Train each model on a subset of the data\n",
    "        for ii in range(self.n_models):\n",
    "            print(\"TRAINING MODEL \",ii+1)\n",
    "\n",
    "            # Create training data for the bootstrap\n",
    "            idx_pos_list_shuffled = np.random.permutation(n_all_pos)\n",
    "            idx_neg_list_shuffled = np.random.permutation(n_all_neg)\n",
    "\n",
    "            idx_pos_train = idx_pos_list_shuffled[0:self.n_train_pos]\n",
    "            idx_neg_train = idx_neg_list_shuffled[0:self.n_train_neg]\n",
    "\n",
    "            df_train_pos = df_pos.iloc[idx_pos_train,:]\n",
    "            df_train_neg = df_neg.iloc[idx_neg_train,:]\n",
    "            df_train = pd.concat([df_train_pos,df_train_neg])\n",
    "            df_train = df_train.sort_index()\n",
    "\n",
    "            x_train = np.array(df_train.iloc[:,xlist].values)\n",
    "            y_train = np.array(df_train[target].values)\n",
    "            \n",
    "            # Create the new model and add it to the bootstrap\n",
    "            self.add_model(x_train,y_train)\n",
    "\n",
    "    def add_model(self,x_train,y_train):\n",
    "        model = self.className(C = self.C,gamma = self.gamma,max_iter = self.max_iter)\n",
    "        model.fit(x_train,y_train)\n",
    "        self.models.append(model)\n",
    "        \n",
    "    def predict(self,x_test):\n",
    "        n_data = x_test.shape[0]\n",
    "        y_pred_list = np.mat(np.empty([n_data,self.n_models]))\n",
    "        for ii in range(self.n_models):\n",
    "            print(\"PREDICTING MODEL \",ii+1)\n",
    "            model = self.models[ii]\n",
    "            y_pred_ii = np.mat(model.predict(x_test)).reshape(-1,1)\n",
    "            y_pred_list[:,ii] = y_pred_ii\n",
    "\n",
    "        y_prob = np.sum(y_pred_list,1)\n",
    "        y_pred = np.sign(y_prob)\n",
    "        \n",
    "        return y_prob,y_pred\n",
    "\n",
    "class BootstrapKFolds():\n",
    "    \"\"\"\n",
    "        Bootstrap class that will produce a set of models to average over. \n",
    "        Uses KFolds at each step to choose optimal hyperparameters for each submodel.\n",
    "\n",
    "        Optional Parameters:\n",
    "        n_train_pos = 1000, number of positive examples to include\n",
    "        n_train_neg = 10000, number of negative examples to include\n",
    "        n_models = 15, number of models to create (should be odd)\n",
    "        n_splits = 10, number of splits for k-folds\n",
    "        metric = accuracy_score, metric to use to choose best hyperparameters\n",
    "        className = SVMGaussianKernelModified, name of class to fit\n",
    "        max_iter = 500\n",
    "\n",
    "        Useful functions:\n",
    "        fit(df,xlist,target) = fits a dataframe with X taken from columns in xlist, y has column name target\n",
    "        predict(x_test) = makes an overall prediction for what the final output should be\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,n_models = 15,n_splits = 10, n_samples = 10000, metric = accuracy_score,\\\n",
    "                 className = SVMGaussianKernelModified, gamma_list = [0.1,0.3,1,3,10], C_list = [0.1,0.3,1,3,10], max_iter = 500):\n",
    "        self.n_splits = n_splits\n",
    "        self.n_models = n_models\n",
    "        self.n_samples = n_samples\n",
    "        self.metric = metric\n",
    "        self.className = className\n",
    "        self.gamma_list = gamma_list\n",
    "        self.C_list = C_list\n",
    "        self.max_iter = max_iter\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self,x_train,y_train):\n",
    "        # Split into 2 df with only +/- \n",
    "        # xlist is a list of columns to include in the x table\n",
    "        # target is the name of the row that has the output Y column\n",
    "        \n",
    "        # Train each model on a subset of the data\n",
    "        for ii in range(self.n_models):\n",
    "            print(\"TRAINING MODEL \",ii+1)\n",
    "\n",
    "            # Create training data for the bootstrap\n",
    "            x_sample, y_sample = resample(x_train,y_train,n_samples = self.n_samples)\n",
    "            \n",
    "            # Create the new model and add it to the bootstrap\n",
    "            self.add_model(x_sample,y_sample)\n",
    "\n",
    "    def add_model(self,x_train,y_train):\n",
    "        kfolds = kFoldsTest(x_train,y_train,n_splits = self.n_splits,metric = self.metric)\n",
    "        C_best, gamma_best, model_best = kfolds.get_best(C_list = self.C_list, gamma_list = self.gamma_list, \\\n",
    "                                                className = self.className,max_iter = self.max_iter)\n",
    "        model = self.className(C = C_best,gamma = gamma_best,max_iter = self.max_iter)\n",
    "        model.fit(x_train,y_train)\n",
    "        self.models.append(model)\n",
    "        \n",
    "    def predict(self,x_test):\n",
    "        n_data = x_test.shape[0]\n",
    "        y_pred_list = np.mat(np.empty([n_data,self.n_models]))\n",
    "        for ii in range(self.n_models):\n",
    "            print(\"PREDICTING MODEL \",ii+1)\n",
    "            model = self.models[ii]\n",
    "            y_pred_ii = np.mat(model.predict(x_test)).reshape(-1,1)\n",
    "            y_pred_list[:,ii] = y_pred_ii\n",
    "\n",
    "        y_prob = np.sum(y_pred_list,1)\n",
    "        y_pred = np.sign(y_prob)\n",
    "        \n",
    "        return y_prob,y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniboone Data and Actually Testing Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.954580</td>\n",
       "      <td>-1.189547</td>\n",
       "      <td>-0.581235</td>\n",
       "      <td>0.663547</td>\n",
       "      <td>0.029684</td>\n",
       "      <td>1.737819</td>\n",
       "      <td>-0.486195</td>\n",
       "      <td>1.031514</td>\n",
       "      <td>0.561684</td>\n",
       "      <td>1.724753</td>\n",
       "      <td>-0.392747</td>\n",
       "      <td>-0.258892</td>\n",
       "      <td>0.913123</td>\n",
       "      <td>-0.700593</td>\n",
       "      <td>0.479570</td>\n",
       "      <td>1.163900</td>\n",
       "      <td>1.780980</td>\n",
       "      <td>-0.253765</td>\n",
       "      <td>2.501119</td>\n",
       "      <td>-0.002781</td>\n",
       "      <td>0.884446</td>\n",
       "      <td>0.255506</td>\n",
       "      <td>0.285317</td>\n",
       "      <td>0.734739</td>\n",
       "      <td>0.877964</td>\n",
       "      <td>0.704449</td>\n",
       "      <td>-0.983182</td>\n",
       "      <td>-1.325027</td>\n",
       "      <td>0.295579</td>\n",
       "      <td>-0.585084</td>\n",
       "      <td>-1.124950</td>\n",
       "      <td>-0.276380</td>\n",
       "      <td>-1.098716</td>\n",
       "      <td>0.109303</td>\n",
       "      <td>1.477999</td>\n",
       "      <td>0.216865</td>\n",
       "      <td>2.999088</td>\n",
       "      <td>-1.779542</td>\n",
       "      <td>-1.203942</td>\n",
       "      <td>-1.022458</td>\n",
       "      <td>-1.388995</td>\n",
       "      <td>-0.285413</td>\n",
       "      <td>-0.237741</td>\n",
       "      <td>-0.031755</td>\n",
       "      <td>-0.306696</td>\n",
       "      <td>-1.097718</td>\n",
       "      <td>0.343785</td>\n",
       "      <td>-1.253198</td>\n",
       "      <td>-0.786148</td>\n",
       "      <td>0.289448</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.861168</td>\n",
       "      <td>-0.983886</td>\n",
       "      <td>-0.595089</td>\n",
       "      <td>-0.832887</td>\n",
       "      <td>1.714776</td>\n",
       "      <td>1.630798</td>\n",
       "      <td>0.290185</td>\n",
       "      <td>0.777807</td>\n",
       "      <td>0.545369</td>\n",
       "      <td>0.615331</td>\n",
       "      <td>0.984026</td>\n",
       "      <td>1.445135</td>\n",
       "      <td>0.825367</td>\n",
       "      <td>0.241092</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>0.696496</td>\n",
       "      <td>0.920637</td>\n",
       "      <td>-0.673911</td>\n",
       "      <td>0.183960</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>0.939469</td>\n",
       "      <td>-1.081738</td>\n",
       "      <td>0.210619</td>\n",
       "      <td>0.824826</td>\n",
       "      <td>0.580985</td>\n",
       "      <td>0.126980</td>\n",
       "      <td>-0.956288</td>\n",
       "      <td>-0.961489</td>\n",
       "      <td>-0.573529</td>\n",
       "      <td>0.670441</td>\n",
       "      <td>-1.091118</td>\n",
       "      <td>0.977976</td>\n",
       "      <td>-1.178015</td>\n",
       "      <td>-1.563325</td>\n",
       "      <td>0.208707</td>\n",
       "      <td>0.026132</td>\n",
       "      <td>0.452529</td>\n",
       "      <td>-1.993079</td>\n",
       "      <td>0.545295</td>\n",
       "      <td>-0.557710</td>\n",
       "      <td>1.297546</td>\n",
       "      <td>1.998611</td>\n",
       "      <td>-0.989762</td>\n",
       "      <td>0.022326</td>\n",
       "      <td>-0.266672</td>\n",
       "      <td>-1.081775</td>\n",
       "      <td>-0.313420</td>\n",
       "      <td>-1.003882</td>\n",
       "      <td>-0.652620</td>\n",
       "      <td>-0.001647</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.272820</td>\n",
       "      <td>-0.343170</td>\n",
       "      <td>-0.497763</td>\n",
       "      <td>-1.381345</td>\n",
       "      <td>0.642781</td>\n",
       "      <td>0.813958</td>\n",
       "      <td>0.404516</td>\n",
       "      <td>0.858568</td>\n",
       "      <td>-0.076469</td>\n",
       "      <td>0.074771</td>\n",
       "      <td>0.550914</td>\n",
       "      <td>-0.054313</td>\n",
       "      <td>-0.074502</td>\n",
       "      <td>0.510236</td>\n",
       "      <td>0.192117</td>\n",
       "      <td>0.080932</td>\n",
       "      <td>0.141035</td>\n",
       "      <td>-0.365035</td>\n",
       "      <td>-0.229570</td>\n",
       "      <td>-0.002788</td>\n",
       "      <td>-0.333787</td>\n",
       "      <td>0.073853</td>\n",
       "      <td>-0.172754</td>\n",
       "      <td>-0.220424</td>\n",
       "      <td>0.974367</td>\n",
       "      <td>0.137459</td>\n",
       "      <td>-0.577059</td>\n",
       "      <td>-1.461441</td>\n",
       "      <td>0.341873</td>\n",
       "      <td>-0.345041</td>\n",
       "      <td>-0.997061</td>\n",
       "      <td>-0.031759</td>\n",
       "      <td>0.625887</td>\n",
       "      <td>0.523571</td>\n",
       "      <td>-0.106364</td>\n",
       "      <td>-1.092073</td>\n",
       "      <td>1.170970</td>\n",
       "      <td>0.759908</td>\n",
       "      <td>1.952824</td>\n",
       "      <td>-0.155306</td>\n",
       "      <td>-0.483733</td>\n",
       "      <td>0.299741</td>\n",
       "      <td>-0.841526</td>\n",
       "      <td>0.495954</td>\n",
       "      <td>-0.176214</td>\n",
       "      <td>-0.460923</td>\n",
       "      <td>-0.052562</td>\n",
       "      <td>-0.963231</td>\n",
       "      <td>-0.692448</td>\n",
       "      <td>-0.942995</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.498325</td>\n",
       "      <td>-1.141493</td>\n",
       "      <td>2.963457</td>\n",
       "      <td>-0.020284</td>\n",
       "      <td>-0.010946</td>\n",
       "      <td>-1.308249</td>\n",
       "      <td>0.083398</td>\n",
       "      <td>-0.007295</td>\n",
       "      <td>-1.111718</td>\n",
       "      <td>-0.050817</td>\n",
       "      <td>-0.703059</td>\n",
       "      <td>-1.457519</td>\n",
       "      <td>0.730286</td>\n",
       "      <td>-0.533786</td>\n",
       "      <td>0.353836</td>\n",
       "      <td>0.542512</td>\n",
       "      <td>0.419562</td>\n",
       "      <td>-0.466765</td>\n",
       "      <td>-0.041352</td>\n",
       "      <td>-0.002766</td>\n",
       "      <td>0.424983</td>\n",
       "      <td>-0.049644</td>\n",
       "      <td>0.745244</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.934687</td>\n",
       "      <td>0.447027</td>\n",
       "      <td>-1.144708</td>\n",
       "      <td>0.517562</td>\n",
       "      <td>-0.473309</td>\n",
       "      <td>1.039819</td>\n",
       "      <td>0.148321</td>\n",
       "      <td>1.468028</td>\n",
       "      <td>-0.871474</td>\n",
       "      <td>-0.146526</td>\n",
       "      <td>0.142925</td>\n",
       "      <td>0.627771</td>\n",
       "      <td>-0.845053</td>\n",
       "      <td>-0.743300</td>\n",
       "      <td>-0.127340</td>\n",
       "      <td>0.345128</td>\n",
       "      <td>0.588057</td>\n",
       "      <td>0.096014</td>\n",
       "      <td>-0.228244</td>\n",
       "      <td>-0.773374</td>\n",
       "      <td>-0.306696</td>\n",
       "      <td>0.026812</td>\n",
       "      <td>0.580139</td>\n",
       "      <td>0.772728</td>\n",
       "      <td>-0.581498</td>\n",
       "      <td>0.535463</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.923718</td>\n",
       "      <td>-0.766333</td>\n",
       "      <td>-0.368676</td>\n",
       "      <td>-0.844238</td>\n",
       "      <td>1.261152</td>\n",
       "      <td>0.591967</td>\n",
       "      <td>1.165818</td>\n",
       "      <td>-0.517689</td>\n",
       "      <td>0.827716</td>\n",
       "      <td>0.022032</td>\n",
       "      <td>0.796425</td>\n",
       "      <td>0.156888</td>\n",
       "      <td>-0.100949</td>\n",
       "      <td>0.175730</td>\n",
       "      <td>-0.297215</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>-0.157778</td>\n",
       "      <td>-0.434241</td>\n",
       "      <td>0.245943</td>\n",
       "      <td>-0.002789</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>-1.559278</td>\n",
       "      <td>0.261967</td>\n",
       "      <td>-0.502117</td>\n",
       "      <td>0.407726</td>\n",
       "      <td>0.272457</td>\n",
       "      <td>0.522176</td>\n",
       "      <td>0.469659</td>\n",
       "      <td>-1.010091</td>\n",
       "      <td>0.549665</td>\n",
       "      <td>0.102077</td>\n",
       "      <td>0.779475</td>\n",
       "      <td>-0.712703</td>\n",
       "      <td>-0.510327</td>\n",
       "      <td>-0.742706</td>\n",
       "      <td>-0.673520</td>\n",
       "      <td>-0.879401</td>\n",
       "      <td>-0.408341</td>\n",
       "      <td>1.830943</td>\n",
       "      <td>0.352530</td>\n",
       "      <td>2.652401</td>\n",
       "      <td>1.910015</td>\n",
       "      <td>-1.395880</td>\n",
       "      <td>-0.733926</td>\n",
       "      <td>-0.306696</td>\n",
       "      <td>-0.098825</td>\n",
       "      <td>-0.670245</td>\n",
       "      <td>0.949160</td>\n",
       "      <td>0.158906</td>\n",
       "      <td>0.438961</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.954580 -1.189547 -0.581235  0.663547  0.029684  1.737819 -0.486195   \n",
       "1 -0.861168 -0.983886 -0.595089 -0.832887  1.714776  1.630798  0.290185   \n",
       "2 -1.272820 -0.343170 -0.497763 -1.381345  0.642781  0.813958  0.404516   \n",
       "3 -0.498325 -1.141493  2.963457 -0.020284 -0.010946 -1.308249  0.083398   \n",
       "4  0.923718 -0.766333 -0.368676 -0.844238  1.261152  0.591967  1.165818   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  1.031514  0.561684  1.724753 -0.392747 -0.258892  0.913123 -0.700593   \n",
       "1  0.777807  0.545369  0.615331  0.984026  1.445135  0.825367  0.241092   \n",
       "2  0.858568 -0.076469  0.074771  0.550914 -0.054313 -0.074502  0.510236   \n",
       "3 -0.007295 -1.111718 -0.050817 -0.703059 -1.457519  0.730286 -0.533786   \n",
       "4 -0.517689  0.827716  0.022032  0.796425  0.156888 -0.100949  0.175730   \n",
       "\n",
       "         14        15        16        17        18        19        20  \\\n",
       "0  0.479570  1.163900  1.780980 -0.253765  2.501119 -0.002781  0.884446   \n",
       "1 -0.005146  0.696496  0.920637 -0.673911  0.183960 -0.002774  0.939469   \n",
       "2  0.192117  0.080932  0.141035 -0.365035 -0.229570 -0.002788 -0.333787   \n",
       "3  0.353836  0.542512  0.419562 -0.466765 -0.041352 -0.002766  0.424983   \n",
       "4 -0.297215  0.014324 -0.157778 -0.434241  0.245943 -0.002789  0.032268   \n",
       "\n",
       "         21        22        23        24        25        26        27  \\\n",
       "0  0.255506  0.285317  0.734739  0.877964  0.704449 -0.983182 -1.325027   \n",
       "1 -1.081738  0.210619  0.824826  0.580985  0.126980 -0.956288 -0.961489   \n",
       "2  0.073853 -0.172754 -0.220424  0.974367  0.137459 -0.577059 -1.461441   \n",
       "3 -0.049644  0.745244  0.154300  0.934687  0.447027 -1.144708  0.517562   \n",
       "4 -1.559278  0.261967 -0.502117  0.407726  0.272457  0.522176  0.469659   \n",
       "\n",
       "         28        29        30        31        32        33        34  \\\n",
       "0  0.295579 -0.585084 -1.124950 -0.276380 -1.098716  0.109303  1.477999   \n",
       "1 -0.573529  0.670441 -1.091118  0.977976 -1.178015 -1.563325  0.208707   \n",
       "2  0.341873 -0.345041 -0.997061 -0.031759  0.625887  0.523571 -0.106364   \n",
       "3 -0.473309  1.039819  0.148321  1.468028 -0.871474 -0.146526  0.142925   \n",
       "4 -1.010091  0.549665  0.102077  0.779475 -0.712703 -0.510327 -0.742706   \n",
       "\n",
       "         35        36        37        38        39        40        41  \\\n",
       "0  0.216865  2.999088 -1.779542 -1.203942 -1.022458 -1.388995 -0.285413   \n",
       "1  0.026132  0.452529 -1.993079  0.545295 -0.557710  1.297546  1.998611   \n",
       "2 -1.092073  1.170970  0.759908  1.952824 -0.155306 -0.483733  0.299741   \n",
       "3  0.627771 -0.845053 -0.743300 -0.127340  0.345128  0.588057  0.096014   \n",
       "4 -0.673520 -0.879401 -0.408341  1.830943  0.352530  2.652401  1.910015   \n",
       "\n",
       "         42        43        44        45        46        47        48  \\\n",
       "0 -0.237741 -0.031755 -0.306696 -1.097718  0.343785 -1.253198 -0.786148   \n",
       "1 -0.989762  0.022326 -0.266672 -1.081775 -0.313420 -1.003882 -0.652620   \n",
       "2 -0.841526  0.495954 -0.176214 -0.460923 -0.052562 -0.963231 -0.692448   \n",
       "3 -0.228244 -0.773374 -0.306696  0.026812  0.580139  0.772728 -0.581498   \n",
       "4 -1.395880 -0.733926 -0.306696 -0.098825 -0.670245  0.949160  0.158906   \n",
       "\n",
       "         49  target  \n",
       "0  0.289448     1.0  \n",
       "1 -0.001647     1.0  \n",
       "2 -0.942995     1.0  \n",
       "3  0.535463     1.0  \n",
       "4  0.438961     1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>1.295930e+05</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "      <td>129593.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>8.224318e-20</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.436883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000010</td>\n",
       "      <td>1.000014</td>\n",
       "      <td>1.000012</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>1.000008</td>\n",
       "      <td>0.999845</td>\n",
       "      <td>1.000014</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>1.000002</td>\n",
       "      <td>1.000011</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>1.000015</td>\n",
       "      <td>1.000015</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>0.999790</td>\n",
       "      <td>1.000004e+00</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>1.000006</td>\n",
       "      <td>0.999239</td>\n",
       "      <td>0.999763</td>\n",
       "      <td>1.000012</td>\n",
       "      <td>1.000014</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>0.999781</td>\n",
       "      <td>1.000005</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>1.000011</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>1.000003</td>\n",
       "      <td>1.000014</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>1.000010</td>\n",
       "      <td>1.000011</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.899522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.188448</td>\n",
       "      <td>-1.664962</td>\n",
       "      <td>-0.693401</td>\n",
       "      <td>-2.996159</td>\n",
       "      <td>-13.242265</td>\n",
       "      <td>-1.308249</td>\n",
       "      <td>-2.829028</td>\n",
       "      <td>-11.216474</td>\n",
       "      <td>-4.160400</td>\n",
       "      <td>-3.213869</td>\n",
       "      <td>-12.667532</td>\n",
       "      <td>-1.501883</td>\n",
       "      <td>-2.775347</td>\n",
       "      <td>-1.282772</td>\n",
       "      <td>-1.838525</td>\n",
       "      <td>-1.250410</td>\n",
       "      <td>-1.086072</td>\n",
       "      <td>-0.875248</td>\n",
       "      <td>-4.050953</td>\n",
       "      <td>-2.913166e-03</td>\n",
       "      <td>-12.502507</td>\n",
       "      <td>-5.399574</td>\n",
       "      <td>-0.992591</td>\n",
       "      <td>-1.489452</td>\n",
       "      <td>-7.608762</td>\n",
       "      <td>-18.704597</td>\n",
       "      <td>-1.388984</td>\n",
       "      <td>-3.127112</td>\n",
       "      <td>-3.140437</td>\n",
       "      <td>-5.112476</td>\n",
       "      <td>-2.243555</td>\n",
       "      <td>-6.627726</td>\n",
       "      <td>-1.603528</td>\n",
       "      <td>-3.732081</td>\n",
       "      <td>-1.717701</td>\n",
       "      <td>-3.675139</td>\n",
       "      <td>-2.396146</td>\n",
       "      <td>-10.410457</td>\n",
       "      <td>-3.946031</td>\n",
       "      <td>-2.241845</td>\n",
       "      <td>-3.506912</td>\n",
       "      <td>-10.807094</td>\n",
       "      <td>-1.976373</td>\n",
       "      <td>-1.262598</td>\n",
       "      <td>-0.306696</td>\n",
       "      <td>-2.223482</td>\n",
       "      <td>-32.682844</td>\n",
       "      <td>-8.530110</td>\n",
       "      <td>-8.980860</td>\n",
       "      <td>-4.367990</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.725050</td>\n",
       "      <td>-0.708963</td>\n",
       "      <td>-0.422382</td>\n",
       "      <td>-0.692310</td>\n",
       "      <td>-0.645645</td>\n",
       "      <td>-0.605717</td>\n",
       "      <td>-0.736769</td>\n",
       "      <td>-0.566506</td>\n",
       "      <td>-0.637414</td>\n",
       "      <td>-0.703031</td>\n",
       "      <td>-0.489128</td>\n",
       "      <td>-0.793321</td>\n",
       "      <td>-0.562171</td>\n",
       "      <td>-0.816146</td>\n",
       "      <td>-0.667666</td>\n",
       "      <td>-0.583388</td>\n",
       "      <td>-0.576634</td>\n",
       "      <td>-0.565208</td>\n",
       "      <td>-0.661551</td>\n",
       "      <td>-2.784990e-03</td>\n",
       "      <td>-0.394347</td>\n",
       "      <td>-0.652364</td>\n",
       "      <td>-0.568032</td>\n",
       "      <td>-0.639158</td>\n",
       "      <td>-0.444482</td>\n",
       "      <td>-0.152216</td>\n",
       "      <td>-0.924074</td>\n",
       "      <td>-0.666119</td>\n",
       "      <td>-0.673515</td>\n",
       "      <td>-0.631002</td>\n",
       "      <td>-0.721205</td>\n",
       "      <td>-0.678002</td>\n",
       "      <td>-0.750591</td>\n",
       "      <td>-0.627781</td>\n",
       "      <td>-0.769890</td>\n",
       "      <td>-0.688378</td>\n",
       "      <td>-0.689050</td>\n",
       "      <td>-0.698165</td>\n",
       "      <td>-0.650678</td>\n",
       "      <td>-0.643017</td>\n",
       "      <td>-0.695843</td>\n",
       "      <td>-0.535720</td>\n",
       "      <td>-0.617387</td>\n",
       "      <td>-0.565796</td>\n",
       "      <td>-0.306696</td>\n",
       "      <td>-0.699946</td>\n",
       "      <td>-0.181111</td>\n",
       "      <td>-0.745557</td>\n",
       "      <td>-0.679124</td>\n",
       "      <td>-0.624031</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.060762</td>\n",
       "      <td>-0.124132</td>\n",
       "      <td>-0.116220</td>\n",
       "      <td>-0.181683</td>\n",
       "      <td>-0.344088</td>\n",
       "      <td>-0.157263</td>\n",
       "      <td>-0.134300</td>\n",
       "      <td>0.146734</td>\n",
       "      <td>-0.090619</td>\n",
       "      <td>-0.071935</td>\n",
       "      <td>-0.088358</td>\n",
       "      <td>-0.080443</td>\n",
       "      <td>0.352556</td>\n",
       "      <td>-0.270302</td>\n",
       "      <td>-0.216470</td>\n",
       "      <td>-0.320233</td>\n",
       "      <td>-0.334871</td>\n",
       "      <td>-0.287063</td>\n",
       "      <td>-0.150904</td>\n",
       "      <td>-2.777879e-03</td>\n",
       "      <td>0.171425</td>\n",
       "      <td>-0.028127</td>\n",
       "      <td>-0.310726</td>\n",
       "      <td>-0.187124</td>\n",
       "      <td>0.328037</td>\n",
       "      <td>0.290555</td>\n",
       "      <td>0.110304</td>\n",
       "      <td>-0.022110</td>\n",
       "      <td>-0.169465</td>\n",
       "      <td>0.093279</td>\n",
       "      <td>-0.154763</td>\n",
       "      <td>0.047415</td>\n",
       "      <td>-0.225522</td>\n",
       "      <td>0.201664</td>\n",
       "      <td>-0.137190</td>\n",
       "      <td>-0.052459</td>\n",
       "      <td>-0.216861</td>\n",
       "      <td>0.049925</td>\n",
       "      <td>-0.035374</td>\n",
       "      <td>-0.188597</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>-0.221208</td>\n",
       "      <td>-0.271113</td>\n",
       "      <td>-0.306696</td>\n",
       "      <td>-0.123052</td>\n",
       "      <td>0.207519</td>\n",
       "      <td>-0.254397</td>\n",
       "      <td>-0.246139</td>\n",
       "      <td>0.015999</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.661428</td>\n",
       "      <td>0.532007</td>\n",
       "      <td>0.060448</td>\n",
       "      <td>0.540348</td>\n",
       "      <td>0.309918</td>\n",
       "      <td>0.428579</td>\n",
       "      <td>0.615245</td>\n",
       "      <td>0.715935</td>\n",
       "      <td>0.521321</td>\n",
       "      <td>0.599955</td>\n",
       "      <td>0.338508</td>\n",
       "      <td>0.689877</td>\n",
       "      <td>0.808686</td>\n",
       "      <td>0.657733</td>\n",
       "      <td>0.394081</td>\n",
       "      <td>0.172004</td>\n",
       "      <td>0.128161</td>\n",
       "      <td>0.206261</td>\n",
       "      <td>0.490456</td>\n",
       "      <td>-2.771333e-03</td>\n",
       "      <td>0.558882</td>\n",
       "      <td>0.608572</td>\n",
       "      <td>0.160767</td>\n",
       "      <td>0.401950</td>\n",
       "      <td>0.736891</td>\n",
       "      <td>0.480441</td>\n",
       "      <td>0.600982</td>\n",
       "      <td>0.580982</td>\n",
       "      <td>0.530036</td>\n",
       "      <td>0.667865</td>\n",
       "      <td>0.544210</td>\n",
       "      <td>0.721474</td>\n",
       "      <td>0.510733</td>\n",
       "      <td>0.809732</td>\n",
       "      <td>0.651345</td>\n",
       "      <td>0.633673</td>\n",
       "      <td>0.456673</td>\n",
       "      <td>0.720085</td>\n",
       "      <td>0.591451</td>\n",
       "      <td>0.390446</td>\n",
       "      <td>0.688855</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.343367</td>\n",
       "      <td>0.188626</td>\n",
       "      <td>-0.238976</td>\n",
       "      <td>0.574741</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.482455</td>\n",
       "      <td>0.342999</td>\n",
       "      <td>0.618049</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.500003</td>\n",
       "      <td>8.495147</td>\n",
       "      <td>25.058311</td>\n",
       "      <td>7.617811</td>\n",
       "      <td>13.584222</td>\n",
       "      <td>4.418360</td>\n",
       "      <td>15.369092</td>\n",
       "      <td>2.352994</td>\n",
       "      <td>14.377582</td>\n",
       "      <td>8.030585</td>\n",
       "      <td>15.362117</td>\n",
       "      <td>3.233661</td>\n",
       "      <td>1.049536</td>\n",
       "      <td>2.848565</td>\n",
       "      <td>12.886985</td>\n",
       "      <td>9.272690</td>\n",
       "      <td>8.265165</td>\n",
       "      <td>35.511319</td>\n",
       "      <td>9.430282</td>\n",
       "      <td>3.599889e+02</td>\n",
       "      <td>11.978718</td>\n",
       "      <td>5.985671</td>\n",
       "      <td>15.778247</td>\n",
       "      <td>41.376746</td>\n",
       "      <td>1.024857</td>\n",
       "      <td>2.548779</td>\n",
       "      <td>12.645786</td>\n",
       "      <td>8.031317</td>\n",
       "      <td>8.666831</td>\n",
       "      <td>10.364710</td>\n",
       "      <td>7.084752</td>\n",
       "      <td>8.026342</td>\n",
       "      <td>8.951236</td>\n",
       "      <td>1.378327</td>\n",
       "      <td>7.744678</td>\n",
       "      <td>8.742989</td>\n",
       "      <td>9.132386</td>\n",
       "      <td>3.458074</td>\n",
       "      <td>16.580015</td>\n",
       "      <td>8.308224</td>\n",
       "      <td>5.874979</td>\n",
       "      <td>5.405132</td>\n",
       "      <td>35.052513</td>\n",
       "      <td>17.494582</td>\n",
       "      <td>14.825863</td>\n",
       "      <td>5.309092</td>\n",
       "      <td>2.140008</td>\n",
       "      <td>9.863494</td>\n",
       "      <td>12.141921</td>\n",
       "      <td>7.474287</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1              2              3  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean       -0.000003      -0.000006      -0.000003      -0.000021   \n",
       "std         1.000010       1.000014       1.000012       0.999982   \n",
       "min        -4.188448      -1.664962      -0.693401      -2.996159   \n",
       "25%        -0.725050      -0.708963      -0.422382      -0.692310   \n",
       "50%        -0.060762      -0.124132      -0.116220      -0.181683   \n",
       "75%         0.661428       0.532007       0.060448       0.540348   \n",
       "max        10.500003       8.495147      25.058311       7.617811   \n",
       "\n",
       "                   4              5              6              7  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean       -0.000013       0.000007      -0.000074      -0.000004   \n",
       "std         1.000001       1.000008       0.999845       1.000014   \n",
       "min       -13.242265      -1.308249      -2.829028     -11.216474   \n",
       "25%        -0.645645      -0.605717      -0.736769      -0.566506   \n",
       "50%        -0.344088      -0.157263      -0.134300       0.146734   \n",
       "75%         0.309918       0.428579       0.615245       0.715935   \n",
       "max        13.584222       4.418360      15.369092       2.352994   \n",
       "\n",
       "                   8              9             10             11  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean       -0.000041      -0.000012      -0.000011       0.000018   \n",
       "std         0.999960       1.000002       1.000011       1.000004   \n",
       "min        -4.160400      -3.213869     -12.667532      -1.501883   \n",
       "25%        -0.637414      -0.703031      -0.489128      -0.793321   \n",
       "50%        -0.090619      -0.071935      -0.088358      -0.080443   \n",
       "75%         0.521321       0.599955       0.338508       0.689877   \n",
       "max        14.377582       8.030585      15.362117       3.233661   \n",
       "\n",
       "                  12             13             14             15  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean        0.000026       0.000030      -0.000065       0.000005   \n",
       "std         0.999991       0.999996       0.999922       1.000015   \n",
       "min        -2.775347      -1.282772      -1.838525      -1.250410   \n",
       "25%        -0.562171      -0.816146      -0.667666      -0.583388   \n",
       "50%         0.352556      -0.270302      -0.216470      -0.320233   \n",
       "75%         0.808686       0.657733       0.394081       0.172004   \n",
       "max         1.049536       2.848565      12.886985       9.272690   \n",
       "\n",
       "                  16             17             18            19  \\\n",
       "count  129593.000000  129593.000000  129593.000000  1.295930e+05   \n",
       "mean        0.000002      -0.000062      -0.000099  8.224318e-20   \n",
       "std         1.000015       0.999906       0.999790  1.000004e+00   \n",
       "min        -1.086072      -0.875248      -4.050953 -2.913166e-03   \n",
       "25%        -0.576634      -0.565208      -0.661551 -2.784990e-03   \n",
       "50%        -0.334871      -0.287063      -0.150904 -2.777879e-03   \n",
       "75%         0.128161       0.206261       0.490456 -2.771333e-03   \n",
       "max         8.265165      35.511319       9.430282  3.599889e+02   \n",
       "\n",
       "                  20             21             22             23  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean       -0.000022       0.000055       0.000021      -0.000136   \n",
       "std         0.999978       0.999947       1.000006       0.999239   \n",
       "min       -12.502507      -5.399574      -0.992591      -1.489452   \n",
       "25%        -0.394347      -0.652364      -0.568032      -0.639158   \n",
       "50%         0.171425      -0.028127      -0.310726      -0.187124   \n",
       "75%         0.558882       0.608572       0.160767       0.401950   \n",
       "max        11.978718       5.985671      15.778247      41.376746   \n",
       "\n",
       "                  24             25             26             27  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean        0.000087      -0.000011       0.000009      -0.000009   \n",
       "std         0.999763       1.000012       1.000014       0.999997   \n",
       "min        -7.608762     -18.704597      -1.388984      -3.127112   \n",
       "25%        -0.444482      -0.152216      -0.924074      -0.666119   \n",
       "50%         0.328037       0.290555       0.110304      -0.022110   \n",
       "75%         0.736891       0.480441       0.600982       0.580982   \n",
       "max         1.024857       2.548779      12.645786       8.031317   \n",
       "\n",
       "                  28             29             30             31  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean        0.000053      -0.000049       0.000028       0.000095   \n",
       "std         0.999947       0.999959       0.999995       0.999798   \n",
       "min        -3.140437      -5.112476      -2.243555      -6.627726   \n",
       "25%        -0.673515      -0.631002      -0.721205      -0.678002   \n",
       "50%        -0.169465       0.093279      -0.154763       0.047415   \n",
       "75%         0.530036       0.667865       0.544210       0.721474   \n",
       "max         8.666831      10.364710       7.084752       8.026342   \n",
       "\n",
       "                  32             33             34             35  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean        0.000032      -0.000007       0.000003      -0.000103   \n",
       "std         0.999993       1.000010       1.000010       0.999781   \n",
       "min        -1.603528      -3.732081      -1.717701      -3.675139   \n",
       "25%        -0.750591      -0.627781      -0.769890      -0.688378   \n",
       "50%        -0.225522       0.201664      -0.137190      -0.052459   \n",
       "75%         0.510733       0.809732       0.651345       0.633673   \n",
       "max         8.951236       1.378327       7.744678       8.742989   \n",
       "\n",
       "                  36             37             38             39  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean       -0.000009       0.000076       0.000032       0.000048   \n",
       "std         1.000005       0.999875       0.999983       0.999966   \n",
       "min        -2.396146     -10.410457      -3.946031      -2.241845   \n",
       "25%        -0.689050      -0.698165      -0.650678      -0.643017   \n",
       "50%        -0.216861       0.049925      -0.035374      -0.188597   \n",
       "75%         0.456673       0.720085       0.591451       0.390446   \n",
       "max         9.132386       3.458074      16.580015       8.308224   \n",
       "\n",
       "                  40             41             42             43  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean        0.000027      -0.000011       0.000005       0.000024   \n",
       "std         0.999993       1.000011       1.000010       1.000003   \n",
       "min        -3.506912     -10.807094      -1.976373      -1.262598   \n",
       "25%        -0.695843      -0.535720      -0.617387      -0.565796   \n",
       "50%         0.007634       0.007435      -0.221208      -0.271113   \n",
       "75%         0.688855       0.545073       0.343367       0.188626   \n",
       "max         5.874979       5.405132      35.052513      17.494582   \n",
       "\n",
       "                  44             45             46             47  \\\n",
       "count  129593.000000  129593.000000  129593.000000  129593.000000   \n",
       "mean        0.000007       0.000051      -0.000015      -0.000006   \n",
       "std         1.000014       0.999959       1.000010       1.000011   \n",
       "min        -0.306696      -2.223482     -32.682844      -8.530110   \n",
       "25%        -0.306696      -0.699946      -0.181111      -0.745557   \n",
       "50%        -0.306696      -0.123052       0.207519      -0.254397   \n",
       "75%        -0.238976       0.574741       0.465058       0.482455   \n",
       "max        14.825863       5.309092       2.140008       9.863494   \n",
       "\n",
       "                  48             49         target  \n",
       "count  129593.000000  129593.000000  129593.000000  \n",
       "mean       -0.000111      -0.000027      -0.436883  \n",
       "std         0.999748       0.999990       0.899522  \n",
       "min        -8.980860      -4.367990      -1.000000  \n",
       "25%        -0.679124      -0.624031      -1.000000  \n",
       "50%        -0.246139       0.015999      -1.000000  \n",
       "75%         0.342999       0.618049       1.000000  \n",
       "max        12.141921       7.474287       1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DF \n",
    "\n",
    "# Important Values\n",
    "n_signal = 36499\n",
    "n_bkgd = 93565\n",
    "n_features = 50\n",
    "\n",
    "# Read in all the data \n",
    "data = pd.read_csv('../Data/MiniBooNE_PID.txt', sep='\\s+', skiprows=1,header=None)\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Scale and append signal/background cases\n",
    "scaler = StandardScaler()\n",
    "data = data[data>-999] # Get rid of bad data, set to NA\n",
    "scaler.fit(data)\n",
    "data = pd.DataFrame(scaler.transform(data))\n",
    "\n",
    "data_sig = data.iloc[0:n_signal,:]\n",
    "data_sig.dropna(inplace=True)\n",
    "\n",
    "data_bkgd = data.iloc[n_signal:,:]\n",
    "#data_bkgd = data_bkgd[data_bkgd>-999]\n",
    "data_bkgd.dropna(inplace=True)\n",
    "\n",
    "# Add a column of 1s for signal and 0s for background\n",
    "n_sig_new = data_sig.shape[0]\n",
    "n_bkgd_new = data_bkgd.shape[0]\n",
    "data_sig = data_sig.assign(target=np.ones((n_sig_new,1)))\n",
    "data_bkgd = data_bkgd.assign(target=-np.ones((n_bkgd_new,1)))\n",
    "data_full = data_sig.append(data_bkgd)\n",
    "\n",
    "# All data\n",
    "x_data = data_full.drop(['target'],axis=1).values\n",
    "y_data = data_full['target'].values\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "display(data_full.head())\n",
    "data_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Split\n",
    "df_train, df_test = train_test_split(data_full, shuffle=True, test_size = 0.33)\n",
    "\n",
    "# Train data\n",
    "x_train = df_train.drop(['target'],axis=1).values\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Test Data\n",
    "x_test = df_test.drop(['target'],axis=1).values\n",
    "y_test = df_test['target'].values\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.10 and gamma = 0.10\n"
     ]
    }
   ],
   "source": [
    "# To run k-folds: not needed right now, so leave commented out. \n",
    "# Class name can be the sklearn SVC I think, I'm pretty sure all the variable names are the same. \n",
    "metric = f1_score\n",
    "kfolds = kFoldsTest(x_train,y_train,n_splits = 10,metric = metric)\n",
    "\n",
    "C_list = [0.1,1,10]\n",
    "gamma_list = [0.1,0.3,1,3,10]\n",
    "className = SVC# SVMGaussianKernelModified or SVC\n",
    "max_iter = 7500\n",
    "C_best, gamma_best, model_best = kfolds.get_best(C_list = C_list, gamma_list = gamma_list, \\\n",
    "                                                className = className,max_iter = max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL  1\n",
      "\n",
      "TRAINING MODEL  2\n",
      "\n",
      "TRAINING MODEL  3\n",
      "\n",
      "TRAINING MODEL  4\n",
      "\n",
      "TRAINING MODEL  5\n",
      "\n",
      "TRAINING MODEL  6\n",
      "\n",
      "TRAINING MODEL  7\n",
      "\n",
      "TRAINING MODEL  8\n",
      "\n",
      "TRAINING MODEL  9\n",
      "\n",
      "TRAINING MODEL  10\n",
      "\n",
      "TRAINING MODEL  11\n",
      "\n",
      "TRAINING MODEL  12\n",
      "\n",
      "TRAINING MODEL  13\n",
      "\n",
      "TRAINING MODEL  14\n",
      "\n",
      "TRAINING MODEL  15\n",
      "\n",
      "TRAINING MODEL  16\n",
      "\n",
      "TRAINING MODEL  17\n",
      "\n",
      "TRAINING MODEL  18\n",
      "\n",
      "TRAINING MODEL  19\n",
      "\n",
      "TRAINING MODEL  20\n",
      "\n",
      "TRAINING MODEL  21\n",
      "\n",
      "TRAINING MODEL  22\n",
      "\n",
      "TRAINING MODEL  23\n",
      "\n",
      "TRAINING MODEL  24\n",
      "\n",
      "TRAINING MODEL  25\n",
      "\n",
      "TRAINING MODEL  26\n",
      "\n",
      "TRAINING MODEL  27\n",
      "\n",
      "TRAINING MODEL  28\n",
      "\n",
      "TRAINING MODEL  29\n",
      "\n",
      "TRAINING MODEL  30\n",
      "\n",
      "TRAINING MODEL  31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BOOTSTRAP STUFF\n",
    "target = 'target' # Name of target column\n",
    "xlist = np.arange(1,73)           # Columns used for x\n",
    "\n",
    "n_train_pos = 3000\n",
    "n_train_neg = 7000\n",
    "n_models = 31\n",
    "className = SVMGaussianKernelModified # (OR SVC?)\n",
    "# C_best = 10\n",
    "# gamma_best = 3\n",
    "gamma = gamma_best\n",
    "C = C_best\n",
    "max_iter = -1 # Can be -1 for SVC?\n",
    "\n",
    "# Create boostrap models \n",
    "boot = Bootstrap(n_train_pos = n_train_pos,n_train_neg = n_train_neg,n_models = n_models,\\\n",
    "                className = className, gamma = gamma, C = C, max_iter = max_iter)\n",
    "boot.fit(df_train,xlist = xlist, target = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTING MODEL  1\n",
      "PREDICTING MODEL  2\n",
      "PREDICTING MODEL  3\n",
      "PREDICTING MODEL  4\n",
      "PREDICTING MODEL  5\n",
      "PREDICTING MODEL  6\n",
      "PREDICTING MODEL  7\n",
      "PREDICTING MODEL  8\n",
      "PREDICTING MODEL  9\n",
      "PREDICTING MODEL  10\n",
      "PREDICTING MODEL  11\n",
      "PREDICTING MODEL  12\n",
      "PREDICTING MODEL  13\n",
      "PREDICTING MODEL  14\n",
      "PREDICTING MODEL  15\n",
      "PREDICTING MODEL  16\n",
      "PREDICTING MODEL  17\n",
      "PREDICTING MODEL  18\n",
      "PREDICTING MODEL  19\n",
      "PREDICTING MODEL  20\n",
      "PREDICTING MODEL  21\n",
      "PREDICTING MODEL  22\n",
      "PREDICTING MODEL  23\n",
      "PREDICTING MODEL  24\n",
      "PREDICTING MODEL  25\n",
      "PREDICTING MODEL  26\n",
      "PREDICTING MODEL  27\n",
      "PREDICTING MODEL  28\n",
      "PREDICTING MODEL  29\n",
      "PREDICTING MODEL  30\n",
      "PREDICTING MODEL  31\n",
      "\n",
      "Number of -1 cases =  43390\n",
      "Number of +1 cases =  2029\n",
      "The ratio of Positve to Negative cases is 0.047\n",
      "\n",
      "Number of Negative Classes is quite large. Recommend using the Precision-Recall Metrics.\n",
      "\n",
      "\n",
      "Overall Accuracy = 0.957\n",
      "\n",
      "           CONFUSION MATRIX\n",
      "                 y_pred\n",
      "              -1        1     \n",
      "           --------------------\n",
      "y_test -1 |  41584      1806   |\n",
      "        1 |   131       1898   |\n",
      "           --------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.975\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score = 0.662\n",
      "AUC for Precision-Recall: 0.796\n",
      "Average Precision = 0.703\n"
     ]
    }
   ],
   "source": [
    "# Check out the metrics for the bootstrap\n",
    "y_prob, y_pred = boot.predict(x_test)\n",
    "metrics = ModelMetrics(y_test,y_pred,y_prob)\n",
    "metrics.all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Model, all trained data\n",
    "C_best = 10\n",
    "gamma_best = 3\n",
    "# USE THIS ONE FOR A \"QUICK\" RUN\n",
    "model_train = SVC(C = C_best,gamma = gamma_best,max_iter =-1)\n",
    "model_train.fit(x_train,y_train)\n",
    "y_prob = model_train.decision_function(x_test)\n",
    "y_pred = model_train.predict(x_test)\n",
    "metrics_test = ModelMetrics(y_test,y_pred,y_prob)\n",
    "metrics_test.all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL  1\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 69.91%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.58%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.82%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.37%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.60%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.35%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.98%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  2\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 70.54%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.19%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.81%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.74%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.62%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.96%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.79%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  3\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.99%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.99%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.32%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.44%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.54%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 16.64%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.92%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  4\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.39%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.29%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.37%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.03%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.85%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.35%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.44%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  5\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 70.49%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.78%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.24%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 19.15%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.78%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.44%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.82%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  6\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.07%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.19%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 15.21%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 15.61%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.50%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 15.60%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 15.81%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  7\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.13%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.89%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.76%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.15%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.14%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.12%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.19%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  8\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 70.77%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.23%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 16.32%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 15.39%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.53%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 15.72%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.36%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  9\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.65%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.88%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 16.26%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 15.84%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.39%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 16.50%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.80%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  10\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.73%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.27%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.91%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.37%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 85.86%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.73%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 19.24%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  11\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.37%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.52%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.29%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.04%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.12%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.10%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.51%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  12\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.19%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.28%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.27%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.81%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.27%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.78%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.09%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  13\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.01%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.88%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.41%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.06%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.69%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.98%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.09%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  14\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.78%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.04%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 22.03%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 22.02%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.44%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 20.95%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 21.51%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  15\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 73.65%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.37%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.21%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.59%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.61%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.53%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.20%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  16\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.90%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.07%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 14.74%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 15.01%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 85.91%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 15.27%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 14.95%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  17\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.52%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.43%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.75%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.85%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.15%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.41%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.46%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  18\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.11%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.46%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.47%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.58%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.27%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.91%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.20%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  19\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.09%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.82%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 20.04%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 19.18%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.56%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.50%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 19.03%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  20\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.77%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.34%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.64%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.76%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.69%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 20.06%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 19.38%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  21\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 70.28%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.50%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 16.98%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.47%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.08%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 16.83%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.48%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  22\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.83%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.68%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.07%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.91%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.63%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.90%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.99%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  23\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.68%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.75%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.74%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.75%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.71%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.60%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.63%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  24\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 72.53%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.76%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.40%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.92%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.87%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.50%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.37%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  25\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.25%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.08%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 20.59%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 20.27%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.02%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.84%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 20.38%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  26\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.50%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.59%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.16%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.50%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.33%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 16.59%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.70%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  27\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 70.27%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.87%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 20.78%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 20.71%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.02%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.96%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 19.46%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  28\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.14%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.81%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 16.19%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 16.21%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.30%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 17.01%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 17.18%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  29\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 70.52%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.15%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. \"Accuracy\" was 19.22%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 19.12%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 85.99%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.17%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.60%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  30\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 71.95%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.69%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.38%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.99%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.33%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.56%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 19.06%\n",
      "\n",
      "The best value of C was  1\n",
      "The best value of gamma was  0.1\n",
      "TRAINING MODEL  31\n",
      "C = 0.10 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 73.21%\n",
      "\n",
      "C = 0.10 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 0.10 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 0.00%\n",
      "\n",
      "C = 1.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 86.38%\n",
      "\n",
      "C = 1.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 18.75%\n",
      "\n",
      "C = 1.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 18.96%\n",
      "\n",
      "C = 10.00 and gamma = 0.10\n",
      "Done. \"Accuracy\" was 87.10%\n",
      "\n",
      "C = 10.00 and gamma = 1.00\n",
      "Done. \"Accuracy\" was 19.21%\n",
      "\n",
      "C = 10.00 and gamma = 10.00\n",
      "Done. \"Accuracy\" was 19.24%\n",
      "\n",
      "The best value of C was  10\n",
      "The best value of gamma was  0.1\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap plus k-folds\n",
    "\n",
    "# K-FOLDS STUFF\n",
    "metric = f1_score\n",
    "C_list = [0.1,1,10]\n",
    "gamma_list = [0.1,1,10]\n",
    "max_iter = 7500\n",
    "n_splits = 10\n",
    "\n",
    "# BOOTSTRAP STUFF\n",
    "target = 'target' # Name of target column\n",
    "xlist = np.arange(1,73)           # Columns used for x\n",
    "\n",
    "n_samples = 10000\n",
    "n_models = 31\n",
    "className = SVC# SVMGaussianKernelModified # (OR SVC?)\n",
    "max_iter = -1 # Can be -1 for SVC?\n",
    "\n",
    "# Create boostrap models \n",
    "boot = BootstrapKFolds(n_models = n_models, n_splits = n_splits, n_samples = n_samples,\\\n",
    "                metric = metric, className = className, gamma_list = gamma_list, C_list = C_list, max_iter = max_iter)\n",
    "boot.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTING MODEL  1\n",
      "PREDICTING MODEL  2\n",
      "PREDICTING MODEL  3\n",
      "PREDICTING MODEL  4\n",
      "PREDICTING MODEL  5\n",
      "PREDICTING MODEL  6\n",
      "PREDICTING MODEL  7\n",
      "PREDICTING MODEL  8\n",
      "PREDICTING MODEL  9\n",
      "PREDICTING MODEL  10\n",
      "PREDICTING MODEL  11\n",
      "PREDICTING MODEL  12\n",
      "PREDICTING MODEL  13\n",
      "PREDICTING MODEL  14\n",
      "PREDICTING MODEL  15\n",
      "PREDICTING MODEL  16\n",
      "PREDICTING MODEL  17\n",
      "PREDICTING MODEL  18\n",
      "PREDICTING MODEL  19\n",
      "PREDICTING MODEL  20\n",
      "PREDICTING MODEL  21\n",
      "PREDICTING MODEL  22\n",
      "PREDICTING MODEL  23\n",
      "PREDICTING MODEL  24\n",
      "PREDICTING MODEL  25\n",
      "PREDICTING MODEL  26\n",
      "PREDICTING MODEL  27\n",
      "PREDICTING MODEL  28\n",
      "PREDICTING MODEL  29\n",
      "PREDICTING MODEL  30\n",
      "PREDICTING MODEL  31\n",
      "\n",
      "Number of -1 cases =  30798\n",
      "Number of +1 cases =  11968\n",
      "The ratio of Positve to Negative cases is 0.389\n",
      "\n",
      "Number of Negative Classes is quite large. Recommend using the Precision-Recall Metrics.\n",
      "\n",
      "\n",
      "Overall Accuracy = 0.930\n",
      "\n",
      "           CONFUSION MATRIX\n",
      "                 y_pred\n",
      "              -1        1     \n",
      "           --------------------\n",
      "y_test -1 |  29719      1079   |\n",
      "        1 |   1917     10051   |\n",
      "           --------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.952\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score = 0.870\n",
      "AUC for Precision-Recall: 0.930\n",
      "Average Precision = 0.901\n"
     ]
    }
   ],
   "source": [
    "# Check out the metrics for the bootstrap\n",
    "y_prob, y_pred = boot.predict(x_test)\n",
    "metrics = ModelMetrics(y_test,y_pred,y_prob)\n",
    "metrics.all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
