{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This notebook goes over the problems posed for logistic regression in the accompanying worksheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "---\n",
    "Bernoulli random variable is given by the probability distribution:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Y_i = y_i) = \\pi_i^{y_i} (1 - \\pi_i)^{1-y_i}, \\quad for \\quad y_i = 0,1\n",
    "$$\n",
    "\n",
    "What is the expectation $\\mathbb{E}(Y_i)$ and variance $Var(Y_i)$ of the Bernoulli random variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1\n",
    "---\n",
    "\n",
    "$\\mathbb{E}(Y_i) = \\sum \\limits_{y_i = 0,1} P(Y_i = y_i) y_i = 1\\times \\pi_i + 0\\times(1-\\pi_i) = \\pi_i$\n",
    "\n",
    "$Var(Y_i) = \\mathbb{E}(Y_i^2)-\\mathbb{E}(Y_i)^2 = 1\\times \\pi_i - (\\pi_i)^2 = \\pi_i(1-\\pi_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2\n",
    "---\n",
    "Binomial random variable is given by the probability distribution:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\mathbb{P}(Y_i = y_i) = \\left(\n",
    "  \\begin{array}{rcr}\n",
    "    n_i \\\\\n",
    "    y_i\n",
    "  \\end{array}\n",
    "  \\right) \\pi_i^{y_i} (1 - \\pi_i)^{n_i - y_i}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "What is the expectation $\\mathbb{E}(Y_i)$ and variance $Var(Y_i)$ of the Binomial random variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2\n",
    "---\n",
    "\n",
    "We use indicator variables and linearity of expecation values here. \n",
    "\n",
    "Let $I_{ij}$ = 1 if observation j is a success within group i, and 0 otherwise. The indicator variables follow a Bernoulli distribution with parameter $\\pi_i$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$Y_i = \\sum \\limits_{j=1}^{n_i} I_{ij}$\n",
    "\n",
    "Thus we can write:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathbb{E}(Y_i) &= \\mathbb{E}\\left(\\sum \\limits_{j=1}^{n_i} I_{ij}\\right) \\\\\n",
    "&= \\sum \\limits_{j=1}^{n_i} \\mathbb{E}(I_{ij}) \\\\\n",
    "&= \\sum \\limits_{j=1}^{n_i} \\pi_i \\\\\n",
    "&= n_i \\pi_i\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "We can do the same thing for the variance, but let's work out the first squared term separately. In the following we make use of:\n",
    "\n",
    "* $ \\mathbb{E}(I_{ij} I_{ik}) = \\mathbb{E}(I_{ij})\\mathbb{E}(I_{ik}) $ (Independent observations)\n",
    "\n",
    "* $ \\sum \\limits_{i=1}^{n} A_i^2 $ Has $n^2$ total terms, n of which are diagonal $(A_i^2)$, and $n^2-n$ of which are off diagonal: $A_i A_j$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathbb{E}(Y_i^2) &= \\mathbb{E}\\left[\\left(\\sum \\limits_{j=1}^{n_i} I_{ij}\\right)^2\\right] \\\\\n",
    "&= \\mathbb{E}\\left[\\sum \\limits_{j=1}^{n_i} \\sum \\limits_{k=1}^{n_i} I_{ij} I_{ik}\\right] \\\\\n",
    "&= \\mathbb{E}\\left[\\sum \\limits_{j=1}^{n_i} I_{ij}^2 + \\sum \\limits_{j=1}^{n_i} \\sum \\limits_{k\\neq j} I_{ij} I_{ik}\\right]\\\\\n",
    "&= \\sum \\limits_{j=1}^{n_i} \\mathbb{E}(I_{ij}^2) + \\sum \\limits_{j=1}^{n_i} \\sum \\limits_{k\\neq j} \\mathbb{E}(I_{ij}I_{jk})\\\\\n",
    "&= \\sum \\limits_{j=1}^{n_i} \\mathbb{E}(I_{ij}^2) + \\sum \\limits_{j=1}^{n_i} \\sum \\limits_{k\\neq j} \\mathbb{E}(I_{ij})\\mathbb{E}(I_{jk})\\\\\n",
    "&= n_i \\pi_i + (n_i^2-n_i) \\pi_i^2\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Thus the variance is:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "Var(Y_i) = \\mathbb{E}(Y_i^2) - \\mathbb{E}(Y_i)^2 = n_i \\pi_i + (n_i^2-n_i) \\pi_i^2 - (n_i \\pi_i)^2 = n_i \\pi_i (1-\\pi_i)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q3\n",
    "---\n",
    "\n",
    "Show that the inverse of the $\\text{logit}^{-1}(a) = \\text{logistic}(a) = \\frac{1}{1 + e^{-a}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A3\n",
    "\n",
    "We start with the defintion and go from there:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "logit(x) = log\\left(\\frac{x}{1-x}\\right) &= y \\\\\n",
    "\\frac{x}{1-x} &= e^y\\\\\n",
    "x &= e^y - xe^y \\\\\n",
    "x(1+e^y) &= e^y\\\\\n",
    "x &= \\frac{e^y}{1+e^y} \\\\\n",
    "x &= \\frac{1}{1+e^{-y}}\\\\\n",
    "x &= logit^{-1}(y) = logistic(y)\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q4-7 (Logistic Regression)\n",
    "---\n",
    "Define the probability mass function (PMF), the log-likelihood, gradient, and Hessian of the LL for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4 - 7\n",
    "---\n",
    "\n",
    "We start with the PMF, which is simply given by the binomial distribution: \n",
    "\n",
    "$ P(Y_i = y_i) = {n_i \\choose y_i} \\pi_i^{y_i} (1-\\pi_i)^{n_i-y_i} $\n",
    "\n",
    "For multiple (N) independent observations, this is promoted to:\n",
    "\n",
    "$ P(\\vec{Y} = \\vec{y}) = \\prod \\limits_{i=1}^{N} {n_i \\choose y_i} \\pi_i^{y_i} (1-\\pi_i)^{n_i-y_i}$\n",
    "\n",
    "\n",
    "We also will need the logistic and logit functions:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\eta_i  = x_i^T \\beta = logit(\\pi_i) = log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) \\\\\n",
    "\\pi_i = logistic(\\eta_i) = \\frac{1}{1+e^{-\\eta_i}}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "To get the log-likelihood function:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "l(\\beta) &= log\\left[\\prod \\limits_{i=1}^{N} {n_i \\choose y_i} \\pi_i^{y_i} (1-\\pi_i)^{n_i-y_i}\\right]\n",
    "\\\\\n",
    "&= \\sum \\limits_{i=1}^{N} log{n_i \\choose y_i} + y_i log(\\pi_i) + (n_i-y_i) log(1-\\pi_i)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Ignoring the constant term (which is just 1 in the case $n_i = 1$ anyways, we get:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "l(\\beta) \n",
    "&= \\sum \\limits_{i=1}^{N}  y_i log(\\pi_i) + (n_i-y_i) log(1-\\pi_i)\n",
    "\\\\\n",
    "&= \\sum \\limits_{i=1}^{N} y_i log \\left( \\frac{\\pi}{1-\\pi_i} \\right) + n_i log(1-\\pi_i)\n",
    "\\\\\n",
    "&= \\sum \\limits_{i=1}^{N} y_i \\eta_i + n_i log\\left(1-\\frac{1}{1+e^{-\\eta_i}}\\right)\n",
    "\\\\\n",
    "&= \\sum \\limits_{i=1}^{N} y_i \\eta_i - n_i log\\left(1+e^{\\eta_i}\\right)\n",
    "\\\\\n",
    "&= \\sum \\limits_{i=1}^{N} y_i x_i^T\\beta - n_i log\\left(1+e^{x_i^T\\beta}\\right)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Next we take the gradient of the LL. First we make note of a few useful derivatives:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\pi_i}{\\partial \\eta_i} &= \\pi_i(1-\\pi_i) \\\\\n",
    "\\frac{\\partial \\eta_i}{\\partial \\beta_j} &= \\frac{\\partial \\sum \\limits_{k=1}^N x_{ik} \\beta_k}{\\partial \\beta_j} = x_{ij}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Now applying to the LL:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "l(\\beta) &= \\sum \\limits_{i=1}^{N}  y_i log(\\pi_i) + (n_i-y_i) log(1-\\pi_i)\n",
    "\\\\\n",
    "\\frac{\\partial l}{\\partial \\beta_j} &= \\sum \\limits_{i=1}^{N}  y_i \\frac{1}{\\pi_i}(\\pi_i(1-\\pi_i))x_{ij} \n",
    "+ (n_i-y_i) \\frac{1}{1-\\pi_i}(-\\pi_i(1-\\pi_i))x_{ij}\n",
    "\\\\\n",
    "&= \\sum \\limits_{i=1}^{N} y_i x_{ij} - y_i \\pi_i x_{ij} - n_i \\pi_i x_{ij} + y_i \\pi_i x_{ij}\n",
    "\\\\\n",
    "&= \\sum \\limits_{i=1}^{N} y_i x_{ij} - n_i \\pi_i x_{ij} \n",
    "\\\\\n",
    "&= \\sum \\limits_{i=1}^{N} y_i x_{ij} - n_i \\frac{1}{1+e^{-x_i^T\\beta}} x_{ij} \n",
    "\\\\\n",
    "&= \\sum \\limits_{i=1}^{N} y_i x_{ij} - \\mu_i x_{ij} \n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "In vector form:\n",
    "\n",
    "$\\nabla_\\beta l = X^T (\\vec{y} - \\vec{\\pi}) $\n",
    "\n",
    "Where we have used $n_i \\pi_i = \\mu_i$ at the end. \n",
    "\n",
    "Finally, let's compute the Hessian. This is relatively simple, as only one term in the above still carries any $\\beta$ dependence.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial \\beta_j} &= \\sum \\limits_{i=1}^{N} y_i x_{ij} - n_i \\pi_i x_{ij} \n",
    "\\\\\n",
    "\\frac{\\partial^2 l}{\\partial \\beta_j \\beta_k} &= H_{ij} = - \\sum \\limits_{i=1}^{N}  n_i \\pi_i(1-\\pi_i) x_{ik} x_{ij} \n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$ H =- X^T \\vec{\\pi} .* (1-\\vec{\\pi}) X $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q8 \n",
    "---\n",
    "Perform Newton-Rhapson to find the maximum likelihood estimates (MLE).\n",
    "\n",
    "Most of this is just copied over from the worksheet, with the exception of the actual optimization - here it will be implemented with NR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000e+02</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.569154e+07</td>\n",
       "      <td>37.655000</td>\n",
       "      <td>69742.500000</td>\n",
       "      <td>0.357500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.165832e+04</td>\n",
       "      <td>10.482877</td>\n",
       "      <td>34096.960282</td>\n",
       "      <td>0.479864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.556669e+07</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.562676e+07</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>43000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.569434e+07</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.575036e+07</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>88000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.581524e+07</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            User ID         Age  EstimatedSalary   Purchased\n",
       "count  4.000000e+02  400.000000       400.000000  400.000000\n",
       "mean   1.569154e+07   37.655000     69742.500000    0.357500\n",
       "std    7.165832e+04   10.482877     34096.960282    0.479864\n",
       "min    1.556669e+07   18.000000     15000.000000    0.000000\n",
       "25%    1.562676e+07   29.750000     43000.000000    0.000000\n",
       "50%    1.569434e+07   37.000000     70000.000000    0.000000\n",
       "75%    1.575036e+07   46.000000     88000.000000    1.000000\n",
       "max    1.581524e+07   60.000000    150000.000000    1.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Dataset\n",
    "dataset = pd.read_csv('../Data/social_network_data.csv')\n",
    "p = 2\n",
    "N = 200\n",
    "\n",
    "dataset.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,[2,3]].values\n",
    "y =dataset.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Training Set and Testing Set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "sc_X=StandardScaler()\n",
    "x_train=sc_X.fit_transform(x_train)\n",
    "x_test=sc_X.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEWTON RHAPSON SOLUTION\n",
    "\n",
    "# Need to add my own intercept\n",
    "x_trainNR = np.c_[np.ones(len(x_train)), x_train]\n",
    "x_testNR = np.c_[np.ones(len(x_test)),x_test]\n",
    "\n",
    "# Make everything 2D so we can do matrix math\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "def sigmoid(X,beta):\n",
    "    \n",
    "    return 1/(1+np.exp(-X@beta))\n",
    "\n",
    "def loglikelihood(X,y,beta):\n",
    "    \n",
    "    J= np.sum(y*np.log(sigmoid(X,beta))+(1-y)*np.log(1-sigmoid(X,beta)))\n",
    "    \n",
    "    return J\n",
    "\n",
    "def gradient(X,y,beta):\n",
    "    \n",
    "    dbeta = X.T@(y-sigmoid(X,beta))\n",
    "    \n",
    "    return dbeta\n",
    "\n",
    "def hessian(X,y,beta):\n",
    "    \n",
    "    pivec = sigmoid(X,beta)*(1-sigmoid(X,beta))\n",
    "    \n",
    "    pimat = np.diag(pivec.reshape(-1))\n",
    "    \n",
    "    return -X.T@pimat@X\n",
    "\n",
    "\n",
    "def newtonRhapson(betaInit,X,y,max_iter,gamma):\n",
    "\n",
    "    beta = betaInit\n",
    "    # Initialize cost vector and iteration vector for later plotting\n",
    "    iter_vec = np.array(range(max_iter))\n",
    "    cost_vec = np.zeros(max_iter)\n",
    "\n",
    "    for ii in range(max_iter):\n",
    "        # Calculate the gradient\n",
    "        dbeta = gradient(X,y,beta)\n",
    "\n",
    "        # Hessian inverse\n",
    "        hess = hessian(X,y,beta)\n",
    "        hinv = np.linalg.inv(hess)\n",
    "\n",
    "        # Update beta\n",
    "        beta = beta - gamma * hinv@dbeta\n",
    "\n",
    "        # Calculate the value of the log-likelihood\n",
    "        cost = loglikelihood(X,y,beta)\n",
    "\n",
    "        # Save cost for later plotting\n",
    "        cost_vec[ii] = cost\n",
    "\n",
    "    \n",
    "    # Create this to use later\n",
    "    outputPlt = plt.plot(iter_vec,cost_vec)\n",
    "\n",
    "            \n",
    "    return outputPlt,beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize beta\n",
    "betaInit = np.random.uniform(low=-0.1,high=0.1, size=(p+1,1))\n",
    "\n",
    "# Hyperparameters\n",
    "max_iter = 5000\n",
    "gamma = 1e-1\n",
    "\n",
    "# Perform NR\n",
    "(outputPlt,beta) = newtonRhapson(betaInit,x_trainNR,y_train,max_iter,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton-Rhapson Coefficients:\n",
      "\n",
      "intercept: [-1.21895764]\n",
      "\n",
      "coefficients: [[2.37877157 1.54043509]]\n",
      "\n",
      "Newton-Rhapson Coefficients, Scaled Back:\n",
      "\n",
      "coefficients: [[6.27395904e-02 2.16171077e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Coefficients from NR:\n",
    "print('Newton-Rhapson Coefficients:\\n')\n",
    "print('intercept:', beta[0])\n",
    "print('\\ncoefficients:', beta[1:].T)\n",
    "\n",
    "# Coefficients from NR (rescaled?):\n",
    "print('\\nNewton-Rhapson Coefficients, Scaled Back:\\n')\n",
    "print('coefficients:', beta[1:].T/sc_X.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Logistic Model via Black Box\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "classifier.fit(x_train, y_train.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton-Rhapson Coefficients:\n",
      "\n",
      "intercept: [-1.12355165]\n",
      "\n",
      "coefficients: [[2.04497211 1.33187735]]\n"
     ]
    }
   ],
   "source": [
    "# Black Box coefficients\n",
    "print('Newton-Rhapson Coefficients:\\n')\n",
    "print('intercept:', classifier.intercept_)\n",
    "print('\\ncoefficients:', classifier.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q9\n",
    "---\n",
    "\n",
    "What is the impact on the odds of a purchase with a dollar increase in EstimatedSalary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A9\n",
    "---\n",
    "\n",
    "A dollar increase in EstimatedSalary corresponds to an increase in the odds of $e^{\\beta_{ES}} \\sim 1$%\n",
    "\n",
    "(Where I think that I've changed back to the right scale to answer that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q10\n",
    "---\n",
    "\n",
    "What is the odds of purchase with an age of 38 and estimated salary of \\$60,000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odds are:  0.17361189045402123\n"
     ]
    }
   ],
   "source": [
    "x_new = np.array([[38.0,60000.0]])\n",
    "x_newScaled = sc_X.transform(x_new)\n",
    "\n",
    "x_newNR = np.c_[[1.0], x_newScaled]\n",
    "\n",
    "eta_new = x_newNR@beta\n",
    "\n",
    "odds = np.exp(eta_new)\n",
    "\n",
    "print('The odds are: ',odds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
