{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective of the workshop:\n",
    "\n",
    "1. Derive a linear classifier (logistic regression)\n",
    "2. Derive an objective function (negative log-likelihood)\n",
    "3. Optimize the likelihood to learn the parameters\n",
    "4. Predict the class with the highest probability under the model\n",
    "5. Perform diagnostics to evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With univariate linear regression, the output variable takes continuous values, that is $y \\in \\mathbb{R}$. We are now interested in the case where the output variable takes class labels $y \\in \\{0, 1, \\dots, k\\}$.\n",
    "\n",
    "The example shown below is a classical Convolutional Neural Network (CNN). The objective is that given a new image, label it with the correct class. The softmax layer provides the model with the capability of transforming the output into a probability between 0 and 1.\n",
    "\n",
    "\n",
    "In the case of classifying numbers, we have multiclass classification. However, in the case of a binary output, we arrive at the logistic regression - a valuable component to your machine learning arsenal that is simple, yet highly flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn](../Images/cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM Recap\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From last session we learned the three components of a generalized linear model (GLM):\n",
    "\n",
    "1. Random component: the probability distribution of the response variable ($Y$)\n",
    "2. Systematic component: the linear combination of the covariates ($X$)\n",
    "3. Link function ($\\eta = g(\\mathbb{E}(Y_i))$): specifies the link between random and systematic components or in other words, how the expected value of the response relates to the linear predictor of explanatory variables\n",
    "\n",
    "Recall the linear regression, which has the following form:\n",
    "\n",
    "$$\n",
    "Y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "We found that we could formulate this model as a simple case of the GLM:\n",
    "\n",
    "1. The random component of the response is normally distributed. That is, the errors $e \\sim N(0, \\sigma^2)$.\n",
    "2. The systematic component is $X\\beta$.\n",
    "3. The link function is $\\eta = g(\\mathbb{E}(Y_i)) = \\mathbb{E}(Y_i) = X\\beta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Basics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case where we have a **binary** response\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  y_i =\\begin{cases}\n",
    "    1, & \\text{if it's raining in the way home tonight}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can view $y_i$ as a random variable $Y_i$ that takes the values one and zero with probabilities $\\pi_i$ and $1 - \\pi_i$, respectively. The distribution $Y_i$ is known as a _Bernoulli distribution_ with parameter $\\pi_i$, which can be written as:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Y_i = y_i) = \\pi_i^{y_i} (1 - \\pi_i)^{1-y_i}, \\quad for \\quad y_i = 0,1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 1**: What is the expectation $\\mathbb{E}(Y_i)$ and variance $Var(Y_i)$ of the Bournoulli random variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider the following:\n",
    "\n",
    "$$\n",
    "y_i = \\text{ the number of successes in group } i\n",
    "$$\n",
    "\n",
    "Where we view $y_i$ as the realization of a random variable $Y_i$ that takes the values $1, \\dots, n_i$. If the $n_i$ observations are independent and have the same probability $\\pi_i$ of having the attribute of interest, then the distribution is _Binomial_ with paramters $\\pi_i$ and $n_i$.\n",
    "\n",
    "$$\n",
    "Y_i \\sim Bin(n_i, \\pi_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\mathbb{P}(Y_i = y_i) = \\left(\n",
    "  \\begin{array}{rcr}\n",
    "    n_i \\\\\n",
    "    y_i\n",
    "  \\end{array}\n",
    "  \\right) \\pi_i^{y_i} (1 - \\pi_i)^{n_i - y_i}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION 2**: What is the expectation $\\mathbb{E}(Y_i)$ and variance $Var(Y_i)$ of the Binomial random variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important distribution to consider is the _logistic distribution_, which has the following probability density function (PDF):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\frac{e^{x}}{(1 + e^{x})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the cumulative distribution function (CDF) or **logistic function** is\n",
    "\n",
    "$$\n",
    "F(x) = \\int_{-\\infty}^{a} \\frac{e^{x}}{(1 + e^{x})^2} dx = \\frac{e^{x}}{(1 + e^{x})} = \\frac{1}{1 + e^{-x}}\n",
    "$$.\n",
    "\n",
    "The logistic funcion plays an integral role in the construction of the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logit Transformation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  next  step  in  defining  a  model  for  our  data  concerns  the  systematic\n",
    "structure.   We  would  like  to  have  the  probabilities $\\pi_i$ depend  on  a  vector\n",
    "of observed covariates $\\mathbf{x}_i$. The simplest idea would be to let $\\pi_i$ be a linear\n",
    "function of the covariates, say:\n",
    "\n",
    "$$\n",
    "\\pi_i = \\mathbf{x}_i^T\\mathbf{\\beta},\n",
    "$$\n",
    "\n",
    "where $\\beta$ is a vector of regression coefficients. The concern here is that the response has to be a probability, thereby requiring values between 0 and 1, which cannot be guaranteed by the linear predictor $\\mathbf{x}_i^T\\mathbf{\\beta}$, which takes on any real value. \n",
    "\n",
    "An idea to resolve this issue is to apply a transformation (continuous function) to our linear predictor that will ensure any real value can be mapped to a probability space between 0 and 1 - this is where our logistic function comes in.\n",
    "\n",
    "$$\n",
    "\\pi_i = \\mathbb{P}(Y_i = 1 | X, \\beta) = F(\\mathbf{x}_i^T\\mathbf{\\beta}) = \\text{logistic}(\\mathbf{x}_i^T\\mathbf{\\beta}) = \\frac{1}{1 + exp(-\\mathbf{x}_i^T\\beta)}\n",
    "$$,\n",
    "\n",
    "where $F(.)$ is the cumulative distribution function of the logistic distribution, or in other words, the \"logistic function\". We can also consider the case where we want to explain the log odds given a linear predictor. Under monotonicity conditions, we can use the inverse of the logistic function:\n",
    "\n",
    "$$\n",
    "F^{-1}(\\pi_i) = \\text{logit}(\\pi_i) = \\log \\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = \\mathbf{x}_i^T\\mathbf{\\beta}\n",
    "$$\n",
    "\n",
    "The formulation above enables us to jump from the log-odds back to the probability interchangeably. We  are  now  in  a  position  to  define  the  logistic  regression  model, by assuming  that  the logit of  the  probability $\\pi_i$,  rather  than  the  probability itself, follows a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loglin](../Images/logistic_vs_linear.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Show that the inverse of the $\\text{logit}^{-1}(a) = \\text{logistic}(a) = \\frac{1}{1 + e^{-a}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "* Can we use other CDFs instead of the logistic function?\n",
    "* The logistic regression is a simple case of the perceptron: a single layer neural network!\n",
    "* Interpretation of log-odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our framework of GLM, the logistic regression has the following components:\n",
    "\n",
    "1. Random component: The distribution of Y is assumed to be Binomial($n$,$\\pi$), where $\\pi$ is a probability of \"success\". For simplicity, we can let $Y \\sim Binomial(n=1,\\pi)$.\n",
    "2. Systematic component: X's are explanatory variables (can be continuous, discrete, or both) and are linear in the parameters. Transformation of the X's themselves are allowed like in linear regression; this holds for any GLM.\n",
    "3. Link function: Logit link, $\\eta=\\text{logit}(\\pi)=\\text{log} \\left(\\dfrac{\\pi}{1-\\pi}\\right)$\n",
    "\n",
    "More generally, the logit link models the log odds of the mean, and the mean here is $\\pi$. Binary logistic regression models are also known as logit models when the predictors are all categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Define the probability mass function (PMF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**: Derive the log-likelihood, which should look as follows:\n",
    "\n",
    "$$\n",
    "l(\\beta) \\,=\\, \\sum_{i=1}^N x_i^T\\!\\beta y_i \\, -\\, \\sum_{i=1}^N n_i\\,\\log\\left(1 + e^{x_i^T\\!\\beta}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**: Derive the gradient of the log-likelihood. Your result should look like the following:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \\frac{\\partial l}{\\partial\\beta_j} & = &  \\sum_{i=1}^N y_i x_{ij} \\, - \\, \\sum_{i=1}^N n_i \\left(\\frac{1}{1+e^{x_i^T\\!\\beta}}\\right) e^{x_i^T\\!\\beta}x_{ij} & = & \\sum_{i=1}^N (y_i - \\mu_i)x_{ij}, \\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**: Derive the Hessian of the log-likelihood. Your result should look like the following:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \\frac{\\partial^2 l}{\\partial\\beta_j\\partial\\beta_k}  & = &  -\\sum_{i=1}^N n_ix_{ij}\\,\\frac{\\partial}{\\partial\\beta_k} \\left(\\frac{e^{x_i^T\\beta}}{1+e^{x_i^T\\!\\beta}}\\right) & = &  -\\sum_{i=1}^N n_i \\pi_i(1-\\pi_i)x_{ij}x_{ik}\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8** (Assignment): Perform Newton Raphson to find the Maximum Likelihood Estimates (MLE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    " * Assumptions behind logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the weights in logistic regression differs from the interpretation of the weights in linear regression, since the outcome in logistic regression is a probability between 0 and 1. The weights do not influence the probability linearly any longer. The weighted sum is transformed by the logistic function to a probability. Therefore we need to reformulate the equation for the interpretation so that only the linear term is on the right side of the formula. To be explicity, let's use $\\mathbb{P}(Y_i = 1) = \\pi_i$:\n",
    "\n",
    "$$\n",
    "log\\left(\\frac{\\mathbb{P}(y=1)}{1-\\mathbb{P}(y=1)}\\right)=log\\left(\\frac{\\mathbb{P}(y=1)}{\\mathbb{P}(y=0)}\\right)=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\n",
    "$$\n",
    "\n",
    "Exponentiating both sides leads us to the odds:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbb{P}(y=1)}{1-\\mathbb{P}(y=1)}=odds=exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\right)\n",
    "$$\n",
    "\n",
    "Then we compare what happens when we increase one of the feature values by 1. But instead of looking at the difference, we look at the ratio of the two predictions:\n",
    "\n",
    "$$\n",
    "\\frac{odds_{x_j+1}}{odds}=\\frac{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}(x_{j}+1)+\\ldots+\\beta_{p}x_{p}\\right)}{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}x_{j}+\\ldots+\\beta_{p}x_{p}\\right)}\n",
    "$$\n",
    "\n",
    "We apply the following rule:\n",
    "\n",
    "$$\n",
    "\\frac{exp(a)}{exp(b)}=exp(a-b)\n",
    "$$\n",
    "\n",
    "And we remove many terms:\n",
    "\n",
    "$$\n",
    "\\frac{odds_{x_j+1}}{odds}=exp\\left(\\beta_{j}(x_{j}+1)-\\beta_{j}x_{j}\\right)=exp\\left(\\beta_j\\right)\n",
    "$$\n",
    "\n",
    "In the end, we have something as simple as exp() of a feature weight. A change in a feature by one unit changes the odds ratio (multiplicative) by a factor of exp(Î²j). We could also interpret it this way: A change in xj by one unit increases the log odds ratio by the value of the corresponding weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "* How do we interpret the intercept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000e+02</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.569154e+07</td>\n",
       "      <td>37.655000</td>\n",
       "      <td>69742.500000</td>\n",
       "      <td>0.357500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.165832e+04</td>\n",
       "      <td>10.482877</td>\n",
       "      <td>34096.960282</td>\n",
       "      <td>0.479864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.556669e+07</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.562676e+07</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>43000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.569434e+07</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.575036e+07</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>88000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.581524e+07</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            User ID         Age  EstimatedSalary   Purchased\n",
       "count  4.000000e+02  400.000000       400.000000  400.000000\n",
       "mean   1.569154e+07   37.655000     69742.500000    0.357500\n",
       "std    7.165832e+04   10.482877     34096.960282    0.479864\n",
       "min    1.556669e+07   18.000000     15000.000000    0.000000\n",
       "25%    1.562676e+07   29.750000     43000.000000    0.000000\n",
       "50%    1.569434e+07   37.000000     70000.000000    0.000000\n",
       "75%    1.575036e+07   46.000000     88000.000000    1.000000\n",
       "max    1.581524e+07   60.000000    150000.000000    1.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Dataset\n",
    "dataset = pd.read_csv('data/social_network_data.csv')\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,[2,3]].values\n",
    "y =dataset.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split Training Set and Testing Set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "sc_X=StandardScaler()\n",
    "x_train=sc_X.fit_transform(x_train)\n",
    "x_test=sc_X.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the Logistic Model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: [-0.92688212]\n",
      "coefficient: [[1.86983445 0.9005177 ]]\n"
     ]
    }
   ],
   "source": [
    "print('intercept:', classifier.intercept_)\n",
    "print('coefficient:', classifier.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**: What is the impact on the odds of a purchase with a dollar increase in EstimatedSalary?\n",
    "\n",
    "**Question 10**: What is the odds of purchase with an age of 38 and estimated salary of $60,000?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics & Prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are unable to work with the residuals, as you would with OLS (we are using a binomial link function), a way to analyze classification accuracy and fit is through the _confusion matrix_. An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting the Test Set Result\n",
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Confusion Matrix for Evaluation\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123   6]\n",
      " [ 25  46]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix captures a large amount of information that one can use to examine their model's fit or improve training.\n",
    "\n",
    "* Sensitivity - measures a tests ability to identify positive results.\n",
    "* Specificity - measures a tests ability to identify negative results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cm](../Images/confusion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
